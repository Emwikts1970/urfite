---
title: "Introduction to Econometrics with R"
author: "M. Arnold, M. Schmelzer, A. Gerber"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

<!--Include script for hiding output chunks-->
<script src="js/hideOutput.js"></script>

# Introduction

Hier k√∂nnen wir was zum Projekt schreiben! Worum gehts, wie ist es aufgebaut ...



<br>
<br>
<br>

# Chapter 3
## Section 3.5

### Table 3.1 - Not completed (sig. test)
In order to reproduce the data from table 3.1 you need to download the data from the books source (LINK).
We simply read in the data and then make use of the powerful package dplyr to subset the data.

<div class="fold o">
```{r, echo=T, eval=T}
library(dplyr, quietly=T)
# Read in CPS data (taken from http:// ....)
cps_dat <- read.csv("data/cps_ch3.csv", sep=";", dec=".")
# Get an overview of the data structure
head(cps_dat)
# Take cps_dat %>% group data by gender and year %>% compute the mean, standard deviation
# and number of observations for each group
avgs <- cps_dat %>% group_by(a_sex, year) %>% summarise(mean(ahe12), sd(ahe12), n())
print(avgs, digits=2)
```
</div>

Now that we have the numbers we can compute the difference between both genders.

<div class="fold s o">
```{r, echo=T}
male   <- avgs %>% filter(a_sex == 1) 
female <- avgs %>% filter(a_sex == 2)
colnames(male)   <- c("Sex", "Year", "Y_bar_m", "s_m", "n_m")
colnames(female) <- c("Sex", "Year", "Y_bar_f", "s_f", "n_f")

# Gender gap, standard errors and confidence intervals
gap      <- male$Y_bar_m - female$Y_bar_f
gap_se   <- sqrt(male$s_m^2 / male$n_m + female$s_f^2 / female$n_f)
gap_ci_l <- gap - 1.96 * gap_se
gap_ci_u <- gap + 1.96 * gap_se
result <- cbind(male[,-1], female[,-(1:2)], gap, gap_se, gap_ci_l, gap_ci_u)
print(result, digits = 2)
```
</div>

# Chapter 4
## Linear Regression with One Regressor 

This chapter introduces the simple linear regression model which relates one variable, $X$, to another variable $Y$. If for example a school cuts the class sizes by hiring new teachers, how would this effect the performance of the students? With linear regression we can not only examine whether the class size $\left(X\right)$ does have an impact on the test results $\left(Y\right)$. We can also learn something about the direction and the strength of the effect. 

To start with an easy example consider the following combinations of average test scores and the average student to teacher ratios in some districts:

```{r}
class_data <- data.frame(TestScore           = c(680, 640, 670, 660, 630, 660, 635), 
                         StudentTeacherRatio = c(15, 17, 19, 20, 22, 23.5, 25)
                         )
class_data 
```


If we use a simple linear regression model we assume that the relationship between the two variables can be represented by a straight line ($y = mx + n$). Lets suppose that the "true" function which relates test scores and student to teacher ratio is

$$TestScore = 713 - 3 \times ClassSize.$$

Now, we take a look at the data and add the line from above. 


```{r}
plot(TestScore ~ StudentTeacherRatio, data = class_data, ylim = c(600, 700), xlim = c(10,30))
abline(a = 713, b = -3)
```

We find that our line does not touch any of the points and still we argued that it represents the true relationship. The reason for this is the core problem (and also the right to exist) of statistics, randomness. There are almost always effects which cannot be explained in a deterministic fassion and which excaberate the finding of the "truth". 

In order to account for these differences the regression model is extended by an additional term which covers the purely random effect. This term also accounts for all 


<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.1 </h2>          
<h3 class = "left"> Terminology for the Linear Regression Model with a Single Regressor </h2>

<p> The linear regression model is 

$$Y_i = \beta_0 + \beta_1 X_1 + u_i $$

where

- the subscript i runs over observations, $i = 1$, ..., $n$
- $Y_i$ is the *dependent variable*, the *regressand*, or simply the *left-hand variable*
- $X_i$ is the *independent variable*, the *regressor*, or simply the *left-hand variable*
- $\beta_0 + \beta_1 X$ is *population the regression line* or the *population regression function*
- $\beta_0$ is the *intercept* of the population regression line
- $\beta_1$ is the *slope* of the population regression line
- $u_i$ is the *error term*
</p>
</div>

Now, let us assume that we observed both variables from a couple of classes.

```{r}
class_data <- data.frame(TestScore           = c(680, 640, 670, 660, 630, 660, 635), 
                         StudentTeacherRatio = c(15, 17, 19, 20, 22, 23.5, 25))
class_data 
```

It would be a suprise if all this points would lie on a straight line.


```{r}
lm_model    <- lm(TestScore ~ StudentTeacherRatio, data = class_data)

plot(TestScore ~ StudentTeacherRatio, data = class_data, ylim = c(600, 700), xlim = c(10,30))


abline(lm_model)
```


```{r}
library(AER)                                                    # contains the dataset 
data(CASchools) 

CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
```


```{r Table 4.1, results='hold'}
mean(CASchools$tsratio) 
mean(CASchools$score)
sd(CASchools$tsratio) 
sd(CASchools$score)
quantiles          <- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quantile(CASchools$tsratio, quantiles)
quantile(CASchools$score, quantiles)
```


```{r Figure 4.2, fig.align='center', fig.cap='Figure 4.2'}
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))
```

## The Ordinary Least Squares Estimator (OLS)

\begin{align}
\hat{\beta_1} & =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2} \\

\hat{\beta_0} & =  \bar{Y} - \hat{\beta_1} \bar{X} 
\end{align}


Let us apply the formulas above to the model ... and add the regression line to our plot (figure 4.2).

```{r}
attach(CASchools)
beta_1 <- sum((tsratio - mean(tsratio))*(score - mean(score))) / sum((tsratio - mean(tsratio))^2)
beta_0 <- mean(score) - beta_1 * mean(tsratio)
```

```{r, fig.align='center', echo=-1, fig.cap='Figure 4.3'}
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))

abline(a = beta_0, b = beta_1) # add regression line
```

There is a function called `lm` (**l**inear **m**odel) that can compute the OLS for you. It also takes care of a lot of other things like the residuals, fitted values, etc. Use `lm` to compute the OLS and add them to the figure 4.2 as well to check whether the coefficients really are the same.

```{r}
linear_model <- lm(score ~ tsratio, data = CASchools)
linear_model
```

```{r Figure 4.3, fig.align='center', echo=3, fig.cap='Figure 4.4'}
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))
abline(a = beta_0, b = beta_1, lwd=2)
abline(linear_model, lty=2, col="red")
```

# Chapter 5

In this chapter, we continue with the treatment of the simple linear regression model. We will see how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty.
<br>
This chapter covers:

- Testing Hypotheses about regression coefficients

- Confidence intervals for regression coefficients

- Regression when $X$ is a dummy variable

- Heteroskedasticity and Homoskedasticity

## Section 5.1 Two-Sided Hypotheses Concerning $\beta_1$

Using the fact that $\hat{\beta}_1$ is approximately normal distributed in large samples, hypothesis testing about the true value $\beta_1$ can be done with the same approach as discussed in chapter 3.2.

Remember from section 3.2 that a general $t$-statistic has the form

\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]

For testing the hypothesis $H_0: \beta_1 = \beta_{1,0}$, we need to perform the following steps:

1. Compute the standard error of $\hat{\beta}_1$, $SE(\hat{\beta}_1)$

\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}
\]

2. Compute the $t$-statistic.

\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) } \]

3. Reject at the $5\%$ level if $|t^{act}| > 1.96$ or, equivalently, if the $p$-value is less than 0.05. \[
p \text{-value} = \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| > \left| \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] = \text{Pr}_{H_0} (|t| > |t^{act}|) = 2 \Phi(-|t^{act}|)
\] The last equality holds due to the normal approximation.

Consider again the previous OLS regression stored in `linear_model that gave us the regression line
  \[ \widehat{score} = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times tsratio.  \]
For testing a hypothesis regarding the slope parameter, we need $SE(\hat{\beta}_1)$, the standard error of the respective point estimator. These are presented in parantheses under the respective point estimate. 

How can we get these values using R? By looking at the second column of the coefficients' summary, we discover both values.
In the third column, we find $t^{act}$ for tests of hypotheses $H_0: \beta_0=0$ and $H_0: \beta_1=0$. Furthermore we find corresponding $p$-values in the fourth column.

```{r}
summary(linear_model)$coefficients
```

Let's have a closer look at the test $H_0: \beta_1=0 \, v. \, H_1: \beta_1 \neq 0$. Using our revisited knowledge about $t$-statistics we see that

\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75 \]

What does this tell us about the significance of the estimated coefficient? Using R we may visualise how such a statement can be made:

```{r, fig.align='center'}
# Plot the standard normal density
z <- seq(-6,6,0.01)
tact <- -4.75
plot(z, dnorm(z,0,1), type = "l", col="steelblue", lwd=2, yaxs="i", bty = "n", axes=F, ylab = "", cex.lab=0.7)
axis(1, at = c(0,-1.96,1.96,-tact,tact), cex.axis=0.7)

# Shade the critical regions
polygon(c(-6,seq(-6,-1.96,0.01),-1.96),c(0,dnorm(seq(-6,-1.96,0.01)),0),col='orange')
polygon(c(1.96,seq(1.96,6,0.01),6),c(0,dnorm(seq(1.96,6,0.01)),0),col='orange')

# Add arrows and text indicating critical regions and the p-value
arrows(-3.5,0.2,-2.5,0.02, length = 0.1)
arrows(3.5,0.2,2.5,0.02, length = 0.1)

arrows(-5,0.16,-4.75,0, length = 0.1)
arrows(5,0.16,4.75,0, length = 0.1)

text(-3.5,0.22, labels = "p-value/2", cex = 0.7)
text(3.5,0.22, labels = "p-value/2", cex = 0.7)

text(-5,0.18, labels = expression(t^{act}), cex = 0.7)
text(5,0.18, labels = expression(t^{act}), cex = 0.7)

# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96,1.96), ticksize  = 0.145, lwd = 2, col = "darkred")
rug(c(-tact,tact), ticksize  = -0.0451, lwd = 2, col = "darkgreen")

```

We reject the null hypothesis at the 5\% level since $|t^{act}| > 1.96$ or, alternatively and leading to the same result, $p\text{-value} = 2.78*10^{-6} > 0.05$. Hence, we conclude that the coefficient is significantly different from zero. 

## Section 5.2 Confidence Intervals for Regression Coefficients

As we already know, estimates of regression coefficients $\beta_0$ and $\beta_1$ have sampling uncertainty. Therefore, we will never estimate the exact true value of these parameters from sample data. However, we may construct confidence intervals for the intercept and the slope parameter.

### Confidence intervall for $\beta_i$

Imagine you could draw all possible random samples of given size. The interval that contains the true value $\beta_i$ in $95\%$ of all samples is given by the expression

\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]

Equivalently, this interval can be seen as the set of null hypotheses for which a $5\%$ two-sided hypothesis test cannot reject.

Let us now turn back to the regression model stored in `linear_model`. An easy way to get $95\%$ confidence intervals for $\beta_0$ and $\beta_1$ is to use the function `confint(). We only have to provide a fitted model object as an argument. 

```{r}
confint(linear_model)
```

Let us check if the calculation is done as we expect it to be. For $\beta_1$, according to the formula presented above, interval borders are computed as

\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]

so this actually leads to the same interval. Obviously, this interval does not contain zero what, as we have already seen in section 5.1, leads to rejection of the null hypothesis $\beta_{1,0} = 0$.

## Section 5.3

Instead of using a continuous regressor $X$, we might be interested in running the regression 

\[ Y_i = \beta_0 + \beta_1 D_i + u_i \]

where $D_i$ is binary variable or so-called *dummy variable*. Let us define $D_i$ in the following way:

\[ D_i = \begin{cases}
        1 \ \ \text{if $stratio$ in $i^{th}$ district < 20} \\
        0 \ \ \text{if $stratio$ in $i^{th}$ district $\geq$ 20} \\
      \end{cases} \]

## Section 5.4

How is $SE(\hat{\beta}_1)$ computed here? The `summary` function uses the homoskedasticity-only formula 
  \[ SE(\hat{\beta}_1) = \sqrt{ \overset{\sim}{\sigma}^2_{beta_1} } = \sqrt{ \frac{SER^2}{\sum(X_i - \overline{X})^2} }. \]

```{r Figure 5.2, fig.align='center'}
library(scales)

# Genrate some heteroskedastic data

set.seed(123) 
x <- rep(c(10,15,20,25),each=25)
e <- rnorm(100, sd=12)                
i <- order(runif(100, max=dnorm(e, sd=12))) 
y <- 720 - 3.3 * x + e[rev(i)]

# Plot data and inspect conditional distributions

mod <- lm(y ~ x)
plot(x, y, main="An Example of Heteroskedasticity",
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     cex=0.5, pch=19, xlim=c(8,27), ylim=c(600,710))
abline(mod, col="red")
boxplot(y~x, add=TRUE, at=c(10,15,20,25), col=alpha("gray", 0.4), border="black")
```

```{r}
#if(!require(devtools))
#  install.packages("devtools")
#library(devtools)
#install_github("datacamp/tutorial")
#library(tutorial)
```

We already have assigned the model's summary to `mod`. Now, use your R knowledge to display coefficients and standard errors!

```{r, ex="create_a", type="pre-exercise-code", include=FALSE}
# Available in the workspace when the session initializes
set.seed(123) 
x <- rep(c(10,15,20,25),each=25)
e <- rnorm(100, sd=12)                
i <- order(runif(100, max=dnorm(e, sd=12))) 
y <- 720 - 3.3 * x + e[rev(i)]
mod <- summary(lm(y ~ x))
```

```{r, ex="create_a", type="sample-code", include=FALSE}
# Access coefficients and standard errors

```

```{r, ex="create_a", type="solution", include=FALSE}
# Access coefficients and standard errors
mod$coefficients
```

```{r, ex="create_a", type="sct", eval=FALSE, include=FALSE}
test_output_contains("mod$coefficients", incorrect_msg = "Make sure to use `mod$` !")
success_msg("Econometrics ROXXXXXX111!!!")
```

```{r, ex="create_a", type="hint", eval=FALSE, include=FALSE}
Use the dollar operator.
```

# Chapter 6
## Section 6.3
```{r}
# Multiple Regression - Application to Test Scores and the Student-Teacher Ratio
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
model <- lm(score ~ tsratio + english, data = CASchools)
```

## Section 6.4
Taking the code from Section 6.3 you could simply use `summary(model)` to print the $SER$, $R^2$ and adjusted-$R^2$. You find them at the bottom of the output.
```{r}
summary(model)
```
You can also compute the parameters by hand using the formulas on p. 242.
```{r}
n <- nrow(CASchools)    # number of observations (rows)
k <- 2                  # 2 regressors
y_mean <- mean(CASchools$score)

SSR <- sum( residuals(model)^2 )
TSS <- sum( ( CASchools$score - y_mean )^2 )
ESS <- sum((fitted(model) - y_mean)^2)

SER <- sqrt(1/(n-k-1) * SSR)
Rsq <- 1 - (SSR / TSS)
adj_Rsq <- 1 - (n-1)/(n-k-1) * SSR/TSS
cat(SER, Rsq, adj_Rsq, sep = "    ")
```

## Section 6.7
### Examples of Perfect Multicollinearity

If you use `lm` to estimate a model with a set of regressors that suffer from perfect multicollinearity the system will warn you (_1 not defined because of singularities_) and ignore the regressor(s) which is(are) assumed to be a linear combination of the others.
```{r}
# Example 1
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
CASchools$FracEL    <- CASchools$english/100
model <- lm(score ~ tsratio + english + FracEL, data = CASchools)
summary(model)
```


If you compute OLS by hand you will run into the problem as well but noone is helping you out. The computation simply fails. Why is this the case? Take the following example:

Assume you want to estimate a simple linear regression model with an intercept and one regressor:
    \[ y_i = \beta_0 + \beta_1 x_i + u_i\]
When applying perfect multicollinearity $x$ has to be a linear combination of the other regressors. Since the only other regressor is a constant, $x$ has to be constant as well. When you recap the formula for $\beta_1$, namely
    \[ \hat{\beta_1} =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2}\]
you find the variance of the regressor $x$ in the nominator. Since the variance of a constant is zero, you are not able to compute this fraction. $\hat{\beta}_1$ remains undefined.

<font style="color:#004c93; font-weight:bold;">Note:</font> In this special case the nominator equals zero as well. Can you show that?


```{r}
# Example 2
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
CASchools$NVS      <- ifelse(CASchools$tsratio < 12, 0, 1)      # if tsratio smaller 12, NVS = 0, else NVS = 1
model <- lm(score ~ tsratio + english + NVS, data = CASchools)
summary(model)
```


```{r}
# Example 3
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers  # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2  # average test-score
CASchools$PctES    <- 100 - CASchools$english  # Percentage of english speakers 
model <- lm(score ~ tsratio + english + PctES, data = CASchools)
summary(model)
```


# About Urfite

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.


