---
title: "Introduction to Econometrics with R"
author: "M. Arnold, M. Schmelzer, A. Gerber"
date: "`r Sys.Date()`"
runtime: shiny
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

<!--Include script for hiding output chunks-->
<script src="js/hideOutput.js"></script>

# Introduction

Hier k√∂nnen wir was zum Projekt schreiben! Worum gehts, wie ist es aufgebaut ...

<br>
<br>
<br>

# Chapter 3
## Section 3.5

### Table 3.1 - Not completed (sig. test)
In order to reproduce the data from table 3.1 you need to download the data from the books source (LINK).
We simply read in the data and then make use of the powerful package dplyr to subset the data.

<div class="fold o">
```{r, echo=T, eval=T}
library(dplyr, quietly=T)
# Read in CPS data (taken from http:// ....)
cps_dat <- read.csv("data/cps_ch3.csv", sep=";", dec=".")
# Get an overview of the data structure
head(cps_dat)
# Take cps_dat %>% group data by gender and year %>% compute the mean, standard deviation
# and number of observations for each group
avgs <- cps_dat %>% group_by(a_sex, year) %>% summarise(mean(ahe12), sd(ahe12), n())
print(avgs, digits=2)
```
</div>

Now that we have the numbers we can compute the difference between both genders.

<div class="fold o">
```{r, echo=T}
male   <- avgs %>% filter(a_sex == 1) 
female <- avgs %>% filter(a_sex == 2)
colnames(male)   <- c("Sex", "Year", "Y_bar_m", "s_m", "n_m")
colnames(female) <- c("Sex", "Year", "Y_bar_f", "s_f", "n_f")

# Gender gap, standard errors and confidence intervals
gap      <- male$Y_bar_m - female$Y_bar_f
gap_se   <- sqrt(male$s_m^2 / male$n_m + female$s_f^2 / female$n_f)
gap_ci_l <- gap - 1.96 * gap_se
gap_ci_u <- gap + 1.96 * gap_se
result <- cbind(male[,-1], female[,-(1:2)], gap, gap_se, gap_ci_l, gap_ci_u)
print(result, digits = 2)
```
</div>

# Chapter 4
## Linear Regression with One Regressor 

This chapter introduces the simple linear regression model which relates one variable, $X$, to another variable $Y$. If for example a school cuts the class sizes by hiring new teachers, how would this effect the performance of the students? With linear regression we can not only examine whether the class size $\left(X\right)$ does have an impact on the test results $\left(Y\right)$. We can also learn something about the direction and the strength of this effect. 

To start with an easy example consider the following combinations of average test scores and the average student to teacher ratios in some districts:

```{r}
class_data <- data.frame(TestScore           = c(680, 640, 670, 660, 630, 660, 635), 
                         StudentTeacherRatio = c(15, 17, 19, 20, 22, 23.5, 25)
                         )
class_data 
```


If we use a simple linear regression model we assume that the relationship between the two variables can be represented by a straight line ($y = mx + n$). Lets suppose that the "true" function which relates test scores and student to teacher ratio is

$$TestScore = 713 - 3 \times ClassSize.$$

Let us take a look at the data and the line from above. 

```{r}
plot(TestScore ~ StudentTeacherRatio, data = class_data, ylim = c(600, 700), xlim = c(10,30))
abline(a = 713, b = -3)
```

We find that our line does not touch any of the points and still we argued that it represents the true relationship. The reason for this is the core problem (and also the right to exist) of statistics, randomness. There are almost always effects which cannot be explained in a deterministic fassion and which excaberate the finding of the "truth". 

In order to account for the differences the regression model is extended by an **error term** which covers these random effect. This **error term** generally accounts for all differences between the "true" regression line and the actual observed data. This could next to the pure randomness also be measerment errors or the effect of important but not considered independent variables.   


<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.1 </h3>          
<h3 class = "left"> Terminology for the Linear Regression Model with a Single Regressor </h3>

<p> The linear regression model is 

$$Y_i = \beta_0 + \beta_1 X_1 + u_i $$

where

- the subscript i runs over observations, $i = 1$, ..., $n$
- $Y_i$ is the *dependent variable*, the *regressand*, or simply the *left-hand variable*
- $X_i$ is the *independent variable*, the *regressor*, or simply the *left-hand variable*
- $\beta_0 + \beta_1 X$ is *population the regression line* or the *population regression function*
- $\beta_0$ is the *intercept* of the population regression line
- $\beta_1$ is the *slope* of the population regression line
- $u_i$ is the *error term*
</p>
</div>

##4.2 Estimating the Coefficients of the Linear Regression Model

In practical situations the intercept $\beta_0$ and slope $\beta_1$ of the population regression line are unknown. Therefore, we must use data to estimate the two unknown parameters. We will demonstrate how to do that on a real data example. We want to relate test scores to class sizes in 420 California school restricts. The test score is the districtwide average of reading and math scores for fifth graders. Class size is measured as the number of students divided by the number of teachers (student-teacher ratio). The California Shool dataset is available in the `AER` package. After installing the package with `install.packages(AER)` and attaching it with `Library()` the dataset can be loaded using the `data()` function. 

```{r}
library(AER)                                                    # contains the dataset 
data(CASchools) 
```

With `head()` we get a first overview over our data. This function shows only the first 6 rows of the dataset which prevents an overcrowded console output. 

```{r}
head(CASchools)
```

We find that the dataset consists of plenty variables but the two we are intersted in (average test scores and student-teacher ratio) are not included. However, it is easily possible to calculate both from the existing data. 

```{r}
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
```

The next code chunk reproduces table 4.1 from the text book which summatizes the distribution of test scores and class sizes. In order to have a nice display format we gather all data after the computation in a data frame. 

<div class="fold o">

```{r Table 4.1, results='hold'}
avg_tsratio   <- mean(CASchools$tsratio) 
avg_score     <- mean(CASchools$score)
sd_tsratio    <- sd(CASchools$tsratio) 
sd_tsratio    <- sd(CASchools$score)
quantiles     <- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_tsratio <- quantile(CASchools$tsratio, quantiles)
quant_score   <- quantile(CASchools$score, quantiles)

#Gather everything in a data frame 
DistributionSummary <- data.frame(Average           = c(avg_tsratio, avg_tsratio), 
                                  StandardDeviation = c(sd_tsratio, sd_tsratio), 
                                  quantile          = rbind(quant_tsratio, quant_score)
                                  )
DistributionSummary
```
</div>

Figure 4.2 shows a scatterplot of all observations. We see that the points are scatterd strongly and the correlation is with $-0.23$ rather weak. The task we are facing now is to find the line which fit best to this data. 

<div class="fold o">
```{r Figure 4.2}
cor(CASchools$tsratio, CASchools$score)
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student-Teacher Ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))
```
</div>

## The Ordinary Least Squares Estimator (OLS)

The OLS estimator chooses the regression coefficients so that the estimated regression line is as close as possible to the observed data. Closeness is measured by the sum of the squared mistakes made in predicting $Y$ given $X$. Let $b_0$ and $b_1$ be some estimator of $\beta_0$ and $\beta_1$ then the total squared estimation mistakes can be expressed as: 

$$\sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2$$.

The OLS Estimator is the pair of estimators for intercept and slope which minimizes this expression. 

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.2 </h3>          
<h3 class = "left"> The OLS Estimator, Predicted Values, and Residuals </h3>

<p> The OLS estimators of the slope $\beta_1$ and the intercept $\beta_0$ are

\begin{align}
\hat{\beta_1} & =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2} = \frac{s_{xy}} {s^2_X}  \\
\\
\hat{\beta_0} & =  \bar{Y} - \hat{\beta_1} \bar{X} 
\end{align}


The OLS predicted values $\hat{Y_i}$ and residuals $\hat{u_i}$ are

\begin{align}
\hat{Y_i} & =  \hat{\beta_0} + \hat{\beta_1}X_i\\
\\
\\
\hat{u_i} & =  Y_i - \hat{Y_i} 
\end{align}

The estimated intercept $\left(\hat{\beta_0}\right)$, slope $\left(\hat{\beta_1}\right)$, and residuals $\left(\hat{u_i}\right)$ are computed from a sample of n observations of $X_i$ and $Y_i$, $i$, $...$,  $n$. These are estimates of the unkown true population intercept $\left(\beta_0 \right)$, slope $\left(\beta_1\right)$, and error term $(u_i)$.

</p>
</div>

There are many possible ways to compute $\left(\hat{\beta_0}\right)$ and $\left(\hat{\beta_1}\right)$ in R. We could implement the formulas by using some of R's most basic functions `mean()` and `sum()`. But of course their are also other and even more manual ways to do the same tasks. 


```{r}
attach(CASchools)
beta_1 <- sum((tsratio - mean(tsratio))*(score - mean(score))) / sum((tsratio - mean(tsratio))^2)
beta_0 <- mean(score) - beta_1 * mean(tsratio)
beta_1
beta_0
```


Since OLS is one of the most used methods in statistics there is a function called `lm()` (**l**inear **m**odel) which will do the job automatically. This functions also computes plenty other things e.g. the residuals, fitted values and things we will need later on such as p-values. 

<div class="fold o">
```{r, fig.align='center', echo=-1, fig.cap='Figure 4.3'}
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))

abline(a = beta_0, b = beta_1) # add regression line
```
</div>

There is a function called `lm` (**l**inear **m**odel) that can compute the OLS for you. It also takes care of a lot of other things like the residuals, fitted values, etc. Use `lm` to compute the OLS and add them to the figure 4.2 as well to check whether the coefficients really are the same.
>>>>>>> 79efaa3a816cf4f1cf359ff510a590dfa2019937

```{r}
linear_model <- lm(score ~ tsratio, data = CASchools)
linear_model
```

<div class="fold o">
```{r Figure 4.3, fig.align='center', echo=3, fig.cap='Figure 4.4'}
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))
abline(linear_model)
```
</div>

#The Sampling Distribution of the OLS Estimator 

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.4 </h3>          
<h3 class = "left"> Large Sample Distribution of $\hat{\beta_0}$ and $\hat{\beta_1}$ </h3>

<p> If the least square saaumption in Key Concept 4.3 hold, then in large-samples $\hat{\beta_0}$ and $\hat{\beta_1}$ have a jointly normal sampling distribution. The large-sample normal distribution of $\hat{\beta_1}$ is $N(\beta_1, \sigma^2_\hat{\beta_1})$, where the variance of the distribution, $\sigma^2_\hat{\beta_1}$, is 

\[
\sigma^2_\hat{\beta_1} = \frac{1}{n} \frac{var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  var \left(X_i \right)  \right]^2}
\]

The large-sample normal distribution of $\hat{\beta_0}$ is $N(\beta_0, \sigma^2_\hat{\beta_0})$, where
\[
 \sigma^2_\hat{\beta_0} =  \frac{1}{n} \frac{var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 }\text{, where } H_i = 1 - \left[ \frac{\mu_X} {E \left( X_1^2\right)} \right] X_i.
\]

</p>
</div>



```{r}
library(latex2exp)
#Population Regression
pop_n      <- 1000
x          <- runif(pop_n, min = 0, max = 20)
u          <- rnorm(pop_n, sd = 10)
y          <- -2 + 3.5*x + u
population <- data.frame(x, y)


latex2exp

n <- 100 # sample size
sigma_b1 <- var( ( x - mean(x) ) * u ) / (100 * var(x)^2)
H_i      <- 1 - mean(x) / mean(x^2) * x
sigma_b0 <- var(H_i * u) / (n * mean(H_i^2)^2 )

#Estimation
number_of_estimations <- 500


fit <- matrix(ncol = 2, nrow = number_of_estimations)
for (i in 1:number_of_estimations){
 random_sample  <- population[ sample(1:pop_n, n), ]
 fit[i, ]       <- lm(y ~ x, data = random_sample)$coefficients
}

var(fit[ ,1])
hist(fit[ ,1], main = bquote(The ~ distribution  ~ of ~ 500 ~ beta[1] ~ estimates))

var(fit[ ,2])
hist(fit[ ,2], , main = bquote(The ~ distribution  ~ of ~ 500 ~ beta[2] ~ estimates))
```


# Sensitivity to Outliers

```{r}
set.seed(123)
x     <- sort(runif(15, min = 30, max = 70 ))
y     <- rnorm(15 , mean = 200, sd = 50)
y[12] <- 2000

fit               <- lm(y ~ x)
fitWithoutOutlier <- lm(y[-12] ~ x[-12])

plot(y ~ x)
abline(fit)
abline(fitWithoutOutlier)
```

```{r}
library(ggvis)
library(dplyr)

set.seed(123)
x     <- sort(runif(15, min = 30, max = 70 ))
y     <- rnorm(15 , mean = 200, sd = 50)
df    <- data.frame(y, x)

v <- input_slider(0, 250, value = 0, step = 25, animate = TRUE)
outlier <- function(y, v) y + c(rep(0, 14), v)

df %>% 
  ggvis(x = ~x, y = ~y)  %>% 
  mutate(y = outlier(y, eval(v))) %>% 
  layer_points() %>%
  layer_model_predictions(model  = "lm", formula = y ~ x)
```






# Chapter 5

In this chapter, we continue with the treatment of the simple linear regression model. We will see how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty.
<br>
This chapter covers:

- Testing Hypotheses about regression coefficients

- Confidence intervals for regression coefficients

- Regression when $X$ is a dummy variable

- Heteroskedasticity and Homoskedasticity

## Section 5.1 Two-Sided Hypotheses Concerning $\beta_1$

Using the fact that $\hat{\beta}_1$ is approximately normal distributed in large samples, hypothesis testing about the true value $\beta_1$ can be done with the same approach as discussed in chapter 3.2.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.1 </h3>
<h3 class = "left"> General Form of the $t$-Statistic </h3>

Remember from section 3.2 that a general $t$-statistic has the form

\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]

</div>

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.2 </h3>
<h3 class = "left"> Testing Hyothesis about $\beta_1$ </h3>

For testing the hypothesis $H_0: \beta_1 = \beta_{1,0}$, we need to perform the following steps:

1. Compute the standard error of $\hat{\beta}_1$, $SE(\hat{\beta}_1)$

\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}
\]

2. Compute the $t$-statistic.

\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) } \]

3. Reject at the $5\%$ level if $|t^{act}| > 1.96$ or, equivalently, if the $p$-value is less than 0.05. \[
p \text{-value} = \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| > \left| \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] = \text{Pr}_{H_0} (|t| > |t^{act}|) = 2 \Phi(-|t^{act}|)
\] The last equality holds due to the normal approximation.
</div>

Consider again the previous OLS regression stored in `linear_model` that gave us the regression line
  \[ \widehat{score} = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times tsratio.  \]
For testing a hypothesis regarding the slope parameter, we need $SE(\hat{\beta}_1)$, the standard error of the respective point estimator. These are presented in parantheses under the respective point estimate. 

How can we get these values using R? By looking at the second column of the coefficients' summary, we discover both values.
In the third column, we find $t^{act}$ for tests of hypotheses $H_0: \beta_0=0$ and $H_0: \beta_1=0$. Furthermore we find corresponding $p$-values in the fourth column.

```{r}
summary(linear_model)$coefficients
```

Let's have a closer look at the test $H_0: \beta_1=0 \, v. \, H_1: \beta_1 \neq 0$. Using our revisited knowledge about $t$-statistics we see that

\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75 \]

What does this tell us about the significance of the estimated coefficient? Using R we may visualise how such a statement can be made:

```{r, fig.align='center'}
# Plot the standard normal density
z <- seq(-6,6,0.01)
tact <- -4.75
plot(z, dnorm(z,0,1), type = "l", col="steelblue", lwd=2, yaxs="i", bty = "n", axes=F, ylab = "", cex.lab=0.7)
axis(1, at = c(0,-1.96,1.96,-tact,tact), cex.axis=0.7)

# Shade the critical regions
polygon(c(-6,seq(-6,-1.96,0.01),-1.96),c(0,dnorm(seq(-6,-1.96,0.01)),0),col='orange')
polygon(c(1.96,seq(1.96,6,0.01),6),c(0,dnorm(seq(1.96,6,0.01)),0),col='orange')

# Add arrows and text indicating critical regions and the p-value
arrows(-3.5,0.2,-2.5,0.02, length = 0.1)
arrows(3.5,0.2,2.5,0.02, length = 0.1)

arrows(-5,0.16,-4.75,0, length = 0.1)
arrows(5,0.16,4.75,0, length = 0.1)

text(-3.5,0.22, labels = "p-value/2", cex = 0.7)
text(3.5,0.22, labels = "p-value/2", cex = 0.7)

text(-5,0.18, labels = expression(t^{act}), cex = 0.7)
text(5,0.18, labels = expression(t^{act}), cex = 0.7)

# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96,1.96), ticksize  = 0.145, lwd = 2, col = "darkred")
rug(c(-tact,tact), ticksize  = -0.0451, lwd = 2, col = "darkgreen")

```

We reject the null hypothesis at the 5\% level since $|t^{act}| > 1.96$ or, alternatively and leading to the same result, $p\text{-value} = 2.78*10^{-6} > 0.05$. Hence, we conclude that the coefficient is significantly different from zero. 

## Section 5.2 Confidence Intervals for Regression Coefficients

As we already know, estimates of regression coefficients $\beta_0$ and $\beta_1$ have sampling uncertainty. Therefore, we will never estimate the exact true value of these parameters from sample data. However, we may construct confidence intervals for the intercept and the slope parameter.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.3 </h3>
<h3 class = "left"> Confidence Interval for $\beta_i$ </h3>

Imagine you could draw all possible random samples of given size. The interval that contains the true value $\beta_i$ in $95\%$ of all samples is given by the expression

\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]

Equivalently, this interval can be seen as the set of null hypotheses for which a $5\%$ two-sided hypothesis test cannot reject.
</div>

Let us now turn back to the regression model stored in `linear_model`. An easy way to get $95\%$ confidence intervals for $\beta_0$ and $\beta_1$ is to use the function `confint()`. We only have to provide a fitted model object as an argument. 

```{r}
confint(linear_model)
```

Let us check if the calculation is done as we expect it to be. For $\beta_1$, according to the formula presented above, interval borders are computed as

\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]

so this actually leads to the same interval. Obviously, this interval does not contain zero what, as we have already seen in section 5.1, leads to rejection of the null hypothesis $\beta_{1,0} = 0$.

## Section 5.3 Regression when X is Binary

Instead of using a continuous regressor $X$, we might be interested in running the regression 

\[ Y_i = \beta_0 + \beta_1 D_i + u_i \]

where $D_i$ is binary variable or so-called *dummy variable*. 
For example, we define $D_i$ in the following way:

\[ D_i = \begin{cases}
        1 \ \ \text{if $stratio$ in $i^{th}$ district < 20} \\
        0 \ \ \text{if $stratio$ in $i^{th}$ district $\geq$ 20} \\
      \end{cases} \]

The regression model now is

\[ score_i = \beta_0 + \beta_1 D_i + u_i. \]

Let us see how these data points look like.

```{r, eval=F}
# Create the dummy variable as defined above
for (i in 1:nrow(CASchools)) {
  if (CASchools$tsratio[i] < 20) { 
    CASchools$D[i] <- 1
    } else {
      CASchools$D[i] <- 0
    }
  }

# Plot the data
plot(CASchools$D, CASchools$score, 
     pch=20, cex=0.5 ,col="Steelblue",
     xlab=expression(D[i]), ylab="Test Score",
     main = "Dummy Regression"
     )
```

```{r, fig.align='center', echo=F}
# Create the dummy variable as defined above
for (i in 1:nrow(CASchools)) {
  if (CASchools$tsratio[i] < 20) { 
    CASchools$D[i] <- 1
    } else {
      CASchools$D[i] <- 0
    }
  }
dummy_model <- lm(score ~ D, data = CASchools)
# Plot the data
plot(CASchools$D, CASchools$score, 
     pch=20, cex=0.5 ,col="Steelblue",
     xlab=expression(D[i]), ylab="Test Score",
     main = "Dummy Regression"
     )
points(CASchools$D, predict(dummy_model), col="red", pch=20)
```

We see that it is not useful to think of $\beta_1$ as a slope parameter since $D_i \in \{0,1\}$, i.e. we only observe two discrete values instead of continuous regressor values lying (in some range) on the real line. Simply put, there is no continuous line depicting the conditional expectation function $E(score | D_i)$ since this function is solely defined for positions $0$ and $1$.

The interpretation of estimated coefficients from such a regression model is as follows:

- $E(Y_i | D_i = 0) = \beta_0$ so $\beta_0$ is the expectation of $score$ in districts where $D_i=0$ i.e. where $tsratio$ is below $20$.

- $E(Y_i | D_i = 1) = \beta_0 + \beta_1$ or, using the result above, $\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)$. Thus, $\beta_1$ is the difference in group specific expectations, i.e. the difference in expected $score$ between districts where $tsratio < 20$ and those with $tsratio \geq 20$. 

We will now use R to estimate the dummy regression.

```{r}
dummy_model <- lm(score ~ D, data = CASchools)
summary(dummy_model)
```

One can see that the expected test score in districts with $tsratio < 20$ ($D_i = 1$) is predicted to be $650.1 + 7.17 = 657.27$ while districs with $tsratio \geq 20$ ($D_i = 0$) are expected to have an average test score of only $650.1$.

Group specific predictions can be added to the plot by execution of the following code chunk.
```{r, eval=F}
points(CASchools$D, predict(dummy_model), col="red", pch=20)
```
The red dots represent group averages in the sample. Accordingly, $\hat{\beta}_1$ can be seen as the difference in group averages.

By inspection of the output generated with `summary(dummy_model)` we may also find an awnser to the question wether there is a significant difference in group means. This can be assessed by a (two-tailed) test of the hypothesis $H_0: \beta_1 = 0$ for which the $t$-statistic and the corresponding $p$-value are computed defaultly by `summary()`.

Since $t = 3.88 > 1.96$ we reject the null hypothesis at the $5\%$ level of significance. The same conclusion is drawn when using the $p$-value indicating significance to the $0.00012\%$ level. 

Equivalently, we may use the `confint()` function to compute a $95\%$ confidence interval for the true difference in means and see if the hypothesised value is element of this set. 

```{r}
confint(dummy_model)
```

We reject the hypothesis at the $5\%$ significance level since $\beta_{1,0} = 0$ lies outside of the interval $[3.54, 10.8]$.

## Section 5.4 Heteroskedasticity and Homoskedasticity

All inference made in previous subsections of Chapter 5 relies on the assumption that the error variance does not vary as regressor values change. But this will not necessarily be the case in practical applications.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.4 </h3>          
<h3 class = "left"> Heteroskedasticity and Homoskedasticity </h3>

- We say the error term of our regression model is homoskedastic if the variance of the conditional distribution of $u_i$ given $X_i$, $var(u_i|X=x)$ is constant for all observations in our sample
\[ var(u_i|X=x) = \sigma^2 \ \forall \ i=1,\dots,n \]

- If instead there is dependence of the conditional variance of $u_i$ on $X_i$, the error term is said to be heteroskedastic. We then write
\[  var(u_i|X=x) = \sigma_i^2 \ , \ i=1,\dots,n \]

- Homoskedasticity is a special case of heteroskedasticity

For a better understanding of heteroskedasticity, we generate heteroskedastic data, estimate a linear regression model and then use boxplots for illustration of conditional distrubutions of residuals.

<div class="fold o">
```{r, fig.align='center'}
library(scales)

# Genrate some heteroskedastic data

set.seed(123) 
x <- rep(c(10,15,20,25),each=25)
e <- rnorm(100, sd=12)                
i <- order(runif(100, max=dnorm(e, sd=12))) 
y <- 720 - 3.3 * x + e[rev(i)]

# Plot data and inspect conditional distributions

mod <- lm(y ~ x)
plot(x, y, main="An Example of Heteroskedasticity",
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     cex=0.5, pch=19, xlim=c(8,27), ylim=c(600,710))
abline(mod, col="red")
boxplot(y~x, add=TRUE, at=c(10,15,20,25), col=alpha("gray", 0.4), border="black")
```
</div>
In this example, it is straightforward to see that we face unequal conditional variances. Specifically, the variance of the test score (and therefore $var(u_i|X=x)$) increases with $X$.
</div>

###Should we care about heteroskedasticity?

First, let us see how $SE(\hat{\beta}_1)$ is computed under the assumption of homoskedasticity. The `summary` function uses the homoskedasticity-only formula 
  \[ SE(\hat{\beta}_1) = \sqrt{ \overset{\sim}{\sigma}^2_{beta_1} } = \sqrt{ \frac{SER^2}{\sum(X_i - \overline{X})^2} }. \]

This in fact is an estimator for the standard deviation of the estimator $\hat{\beta}_1$ that is inconsistent for the true value $SE(\hat{\beta}_1)$. The implication is that $t$-statistics computed in the manner of key concept 5.1 do not have a standard normal distribution, even in large samples. This issue may invalidate inference drawn from the previously treated tools for hypothesis testing.


We will now use R to compute the homoskedasticity-only standard error estimator for $\hat{\beta}_1$ by hand and see if it matches the value provided by `summary()`.
```{r}
# Store model summary in 'mod'
model <- summary(linear_model)

# Extract the standard error of the regression from model summary
SER <- model$sigma

# Compute the variation in 'tsratio'
V <- (nrow(CASchools)-1)*var(CASchools$tsratio)

# Compute the standard error of the slope parameter's estimator and print it
SE.beta_1.hat <- sqrt(SER^2/V); SE.beta_1.hat

# Use logical operators to see if the value computed by hand matches the one provided 
# in mod$coefficients. Round estimates to four decimal places.

round(model$coefficients[2,2],4) == round(SE.beta_1.hat,4)

```
Indeed, the estimated values are equal.

### Computation of heteroskedasticity-robust standard errors

Cosistent estimation of $SE(\hat{\beta}_1)$ under heteroskedasticity is granted when the following robust estimator is used.

\[ SE(\hat{\beta}_1) = \sqrt{ \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \]

Stadard error estimates computed this way are also referred to as Eicker-Huber-White standard errors. 

It can be quite cumbersome to do so this calculation by hand. Luckily, there are R function for that purpose. A convenient one, named `vcovHC` is contained in the `sandwich` package. 

Let us now compute robust standard error estimates for coefficients in `linear_model`.
```{r}
library(sandwich)

vcov <- vcovHC(linear_model, type = "HC0")
vcov
```
The output is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix since these are the standard error estimates we seek:

```{r}
robust_se <- sqrt(diag(vcov))
robust_se
```


Now assume we want to generate a coefficient summary that reports robust standard error estimates for coefficient estimates, robust $t$-statistics and corresponding $p$-values for the regression model `linear_model`. This can be done as follows:

```{r}
# We invoke the function `coeftest()` on our model. Further we specify in the argument `vcov.`
# that the Eicker-Huber-White estimator, "HC0", is to be used.

coeftest(linear_model, vcov. = vcovHC(linear_model, type = "HC0"))
```

We see that values reported in the column 'Std. Error' equal the ones received using `sqrt(diag(vcov))`.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.5 </h3>          
<h3 class = "left"> The Gauss-Markov-Theorem for $\hat{\beta}_1$ </h3>

Suppose the Assumptions made in Key Concept 4.3 hold *and* that the errors are *homoskedastic*. The OLS estimator is the best (in the sense of smallest variance) linear conditionally unbiased estimator (*BLUE*) in this setting.

Let us have a closer look at what this means:

- Estimators of $\beta_1$ that are linear functions of the $Y_1, \dots, Y_n$ and that are unbiased conditionally on the regressor $X_1, \dots, X_n$ can be written as \[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \] where the $a_i$ are weights that are allowed to depend on the $X_i$ but *not* on the $Y_i$. 

- We already know that $\overset{\sim}{\beta}_1$ has a sampling distribution. If now \[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1 \] we say that $\overset{\sim}{\beta}_1$ is a linear unbiased estimator of $\beta_1$, conditionally on the $X_1, \dots, X_n$.

- We may ask if $\overset{\sim}{\beta}_1$ is also the *best* estimator in this class, i.e. the most efficient one. The weights $a_i$ play an important role here.

#### Example

Consider the case of a regression of $Y_i,\dots,Y_n$ on a constant. Here, the $Y_i$ are assumed to be a random sample from a population with mean $\mu$ and variance $\sigma^2$. We know that the OLS estimator is the sample mean: \[ \hat{\beta}_1 = \overline{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i \]

Clearly, each observation is weighted by $a_i = \frac{1}{n}$.

We also know that $Var(\hat{\beta}_1)=\frac{\sigma^2}{n}$.

We will now use R for a simulation study to illustrate what happens to the variance of the estimator if different weights \[ w_i = \frac{1 \pm \epsilon}{n} \] are assigned to either half of the sample $Y_1, \dots, Y_n$.

```{r, fig.align='center'}

# Set sample size and number of repititionas

n <- 100      
reps <- 1e6

# Choose epsilon and create a vector weights as defined above

epsilon <- 0.8
w <- c( rep((1+epsilon)/n,n/2), rep((1-epsilon)/n,n/2) )

# Draw random sample y_1,...,y_n from the standard normal distribution 
# Compute both estimates 1e6 times  

ols <- rep(NA,reps)
weightedestimator <- rep(NA,reps)

for (i in 1:reps)
{
  y <- rnorm(n)
  ols[i] <- mean(y)
  weightedestimator[i] <- crossprod(w,y)
}

# Plot estimates of the estimators distribution 

plot(density(ols),col="purple", lwd=3, main="Density of OLS and Weighted Estimator",xlab="")
lines(density(weightedestimator),col="steelblue", lwd=3) 
abline(v=0,lty=2)
legend('topright', c("OLS","weighted"), col=c("purple","steelblue"),lwd=3)
```

What conclusion can we draw from the result?

- Both estimators seem to be unbiased: The means of their estimated distributions are zero.
- The `weightedestimator` is less efficient than the `ols` estimator: There is higher dispersion when weights are $w_i = \frac{1 \pm \epsilon}{n}$ instead of $w_i=\frac{1}{n}$. 

<div>


# Chapter 6
## Section 6.3
```{r}
# Multiple Regression - Application to Test Scores and the Student-Teacher Ratio
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
model <- lm(score ~ tsratio + english, data = CASchools)
```

## Section 6.4
Taking the code from Section 6.3 you could simply use `summary(model)` to print the $SER$, $R^2$ and adjusted-$R^2$. You find them at the bottom of the output.
```{r}
summary(model)
```
You can also compute the parameters by hand using the formulas on p. 242.
```{r}
n <- nrow(CASchools)    # number of observations (rows)
k <- 2                  # 2 regressors
y_mean <- mean(CASchools$score)

SSR <- sum( residuals(model)^2 )
TSS <- sum( ( CASchools$score - y_mean )^2 )
ESS <- sum((fitted(model) - y_mean)^2)

SER <- sqrt(1/(n-k-1) * SSR)
Rsq <- 1 - (SSR / TSS)
adj_Rsq <- 1 - (n-1)/(n-k-1) * SSR/TSS
cat(SER, Rsq, adj_Rsq, sep = "    ")
```

## Section 6.7
### Examples of Perfect Multicollinearity

If you use `lm` to estimate a model with a set of regressors that suffer from perfect multicollinearity the system will warn you (_1 not defined because of singularities_) and ignore the regressor(s) which is(are) assumed to be a linear combination of the others.
```{r}
# Example 1
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
CASchools$FracEL    <- CASchools$english/100
model <- lm(score ~ tsratio + english + FracEL, data = CASchools)
summary(model)
```


If you compute OLS by hand you will run into the problem as well but noone is helping you out. The computation simply fails. Why is this the case? Take the following example:

Assume you want to estimate a simple linear regression model with an intercept and one regressor:
    \[ y_i = \beta_0 + \beta_1 x_i + u_i\]
When applying perfect multicollinearity $x$ has to be a linear combination of the other regressors. Since the only other regressor is a constant, $x$ has to be constant as well. When you recap the formula for $\beta_1$, namely
    \[ \hat{\beta_1} =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2}\]
you find the variance of the regressor $x$ in the nominator. Since the variance of a constant is zero, you are not able to compute this fraction. $\hat{\beta}_1$ remains undefined.

<font style="color:#004c93; font-weight:bold;">Note:</font> In this special case the nominator equals zero as well. Can you show that?


```{r}
# Example 2
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
CASchools$NVS      <- ifelse(CASchools$tsratio < 12, 0, 1)      # if tsratio smaller 12, NVS = 0, else NVS = 1
model <- lm(score ~ tsratio + english + NVS, data = CASchools)
summary(model)
```


```{r}
# Example 3
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  <- CASchools$students/CASchools$teachers  # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2  # average test-score
CASchools$PctES    <- 100 - CASchools$english  # Percentage of english speakers 
model <- lm(score ~ tsratio + english + PctES, data = CASchools)
summary(model)
```

<div class="keyconcept">
## Excursus: The Weak Law of Large Numbers

```{r, echo = FALSE}
shinyAppDir(
  "apps/WLLN",
  options=list(
    width="100%", height=500
  )
)
```
</div>


# About Urfite

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.


