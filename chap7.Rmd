This chapter discusses methods that allow to quantify the sampling uncertainty inherent to the OLS estimator by computation of standard errors, statistical hypothesis tests and confidence intervals using basic R functions. We will also tackle the issue of joint hypothesis tests on coefficients in multiple regression models. 

### Hypothesis Tests and Confidence Intervals in Multiple Regression

First, we will see how to compute standard errors, how to test hypotheses and how to construct confidence intervals for regression coefficients in the multiple regression model.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.1 </h3>          
<h3 class = "left"> Testing the Hypothesis $\beta_j = \beta_{j,0}$ <br>
                    Against the Alternative $\beta_j \neq \beta_{j,0}$ </h3>

<p>
1. Compute the standard error of $\hat{\beta_j}$
2. Compute the $t$-statistic,
$$t = \frac{\hat{\beta}_j - \beta_{j,0}} {SE(\hat{\beta_j})}$$
3. Compute the $p$-value,
$$p\text{-value} = 2 \Phi(-|t^{act}|)$$

where $t^{act}$ is the value of the $t$-statistic actually computed. Reject the hypothesis at the 5\% significance level if the $p$-value is less than $0.05$ or, equivalently, if $|t^{act}| > 1.96$. The standard error and (typically) the $t$-statistic and $p$-value testing $\beta_j = 0$ are computed automatically by statistical software.  
</p>
  
</div>

It is straightforward to verify that principles of testing single hypothesis about the significance of coefficients in the multiple regression model are just as in in the simple regression model. 

You can easily see this by inspecting the summary of the regression model

$$ TestScore = \beta_0 - \beta_1 \times size - \beta_2 \times english + u $$

already discussed in chapter 6. Let us review this:

<div class="unfolded">
```{r, echo=5:7, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size  <- CASchools$students/CASchools$teachers
CASchools$score    <- (CASchools$read + CASchools$math)/2

model <- lm(score ~ size + english, data = CASchools)
summary(model)
```
</div>

You may check that calculation of these quantities is done as in the simple regression model by computing the $t$-statistics or $p$-values by hand using the output above and R as a calculator.

E.g. using the definition of the $p$-value for a two sided test, we can confirm the $p$-value for a test of the hypothesis that the coeffiecient on `(intercept)` is zero to be approximatley zero:

<div class="unfolded">
```{r, warning=F, message=F}
2*(1-pnorm(abs(92.566)))
```
</div>

Given a vector of quantiles, `pnorm` calculates associated probabilities for the standard normal distribution by default.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.2 </h3>          
<h3 class = "left"> Confidence Intervals for a Single Coefficient in Multiple Regression </h3>

<p>
A $95\%$ two-sided confidence interval for coefficient $\beta_j$ is an interval that contains the true value of $\beta_j$ with a $95 \%$ probability; that is, it contains the true value of $\beta_j$ in $95 \%$ of all randomly drawn samples. Equivalently, it is the set of values of $\beta_j$ that cannot be rejected by a $5 \%$ two-sided hypothesis test. When the sample size is large, the $95 \%$ confidence interval for $\beta_j$ is
$$\left[\hat{\beta_j}- 1.96 \times SE(\hat{\beta}_j), \hat{\beta_j} + 1.96 \times SE(\hat{\beta_j})\right]$$
</p>
</div>

### An Application to Test Scores and the Student-Teacher Ratio

Let us take a look at the regression from section 6.3 again.

Computing individual confidence intervals for the coefficients in the multiple regression model can be done analogously as in the simple regression model using the function `confint`:

<div class="unfolded">
```{r, warning=F, message=F}
model <- lm(score ~ size + english, data = CASchools)
confint(model)
```
</div>

We note that $95\%$ confidence intervals for all three coefficients are computed. 

If we want to compute confidence intervals at another level of $\alpha$, 0.1 say, we have to set the argument `level` in our call of the `confint` function accordingly.

<div class="unfolded">
```{r, warning=F, message=F}
confint(model, level = 0.9)
```
</div>

The output now provides the desired $90\%$ confidence intervals for all model coefficients.

Knowing how to use R to draw inference about the coefficients in multiple Regression models, you can now answer the following question: 

Can the null hypothesis that a change in the student-teacher ratio (`size`) has no significant effect on test scores (`scores`) --- if we control for the percentage of students learning English in the district (`english`) --- be rejected at the $10\%$ and/or the $5\%$ level of significance?
 
The outputs above tell us that zero is not an element of the computed confidence intervals for the coefficient of `size` such that we can reject the null hypothesis at significance levels of $5\%$ and $10\%$.

Note that rejection at the $5\%$-level implies rejection at the 10\% level (why?). 

The same conclusion can be made when beholding the $p$-value for `size`: $0.00398 < 0.05 = \alpha$.

The $95\%$ confidence interval tells us that we can be $95\%$ confident that a one-unit decrease in the student-teacher ratio has an effect on test scores that lies in the interval $[-1.8487969,  -0.3537944]$.

### Another Augmentation of the Model

What is the effect on test scores of reducing the student-teacher ratio when the expenditures per pupil and the percentage of english learning pupils are held constant?

We can pursue this question by augmenting our model equation yet again and perform *joint* significant testing.

The model we want the estimate now is 

$$ TestScore = \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times expenditure + u $$

with $expenditure$ the total amount of expenditures per pupil in the district (thousands of dollars).

Let us now estimate the model:

<div class="unfolded">
```{r, echo=6:8, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size  <- CASchools$students/CASchools$teachers
CASchools$score    <- (CASchools$read + CASchools$math)/2

CASchools$expenditure <- CASchools$expenditure/1000
model <- lm(score ~ size + english + expenditure, data = CASchools)
summary(model)
```
</div>

We see that the estimated effect of a one unit change in the student-teacher ratio on test scores with expenditures per pupil and the share of english learning pupils held constant is rather small (`-0.29`). What is more, the coefficient of `size` is not significantly different from zero anymore ($p\text{-value}=$`0.55`) so the new model provides no evidence that changing the student-teacher ratio has any effect on the test scores when we do not change expenditures and share of english learners simultaneously.

### Joint Hypothesis Testing Using the $F$-Statistic

Now, can we reject the hypothesis that $\beta_1$ <u>and</u> $\beta_3$ are zero? To answer this question, we have to resort to methods that allow joint testing of two hypotheses. This is different from conducting individual $t$-tests.

The homoskedasticity-only $F$-Statistic is given by

$$ F = \frac{(SSR_{restricted} - SSR_{unrestricted})/q}{SSR_{unrestricted} / (n-k_{unrestricted}-1)} $$

with $SSR_{restricted}$ the sum of squared residuals from the restricted regression, i.e. the regression where we impose the restriction. $SSR_{unrestricted}$ is the sum of squared residuals from the full model, $q$ is the number of restriction under the null and $k$ is the number of regressors in the unrestricted regression.

Luckily, it is fairly easy to conduct an $F$-test in R. We can use the function `linearHypothesis` coming with the `car` package.

<div class="unfolded">
```{r, warning=F, message=F}
model <- lm(score ~ size + english + expenditure, data = CASchools)
# execute the function on the model object and provide the linear restriction to be tested as strings
linearHypothesis(model, c("size=0","expenditure=0"))
```
</div>

From the output we can infer that the $F$-statistic for this joint hypothesis test is around `-8.01` and the corresponding $p$-value is `0.000386`. Thus we can reject the null hypothesis that both coefficients are zero at any level of significance common in practice.

<b>Note</b>:

The standard output of a model summary also reports a related $p$-value corresponding to a $F$-test. The null hypothesis belonging to this quantity is that <u>all</u> population coefficients in the model except for the intercept are zero which is different from testing if only $\beta_1$ and $\beta_3$ are zero.

We will now check that the $F$-statistic belonging to the $p$-values listed in the model's summary coincides with the result reportet by `linearHypothesis`:

<div class="unfolded">
```{r, warning=F, message=F}
# execute the function on the model object and provide the linear restriction to be tested as strings
linearHypothesis(model, c("size=0","english=0","expenditure=0"))
# Access the F-statistic from the model's summary
summary(model)$fstatistic
```
</div>

### Confidence Sets for Multiple Coefficients

Based on the $F$-statistic that we have previously encountered, we can specify confidence sets i.e. sets consisting of combinations of coefficients that contain the true combination of coefficients in, 95\% say, of all cases if we could infinitely draw random samples, just as in the univariate case. Put differently, it is the set of coefficient combinations for which we cannot reject the null hypothesis related to the computed $F$-statistic.  

If we consider two coefficients, their confidence set is an ellipse centered around the coefficient estimates. Again, there is a very convenient way to plot the "confidence ellipse" for coefficients of model objects, namely the function `ellipse` which is also coming with the `car` package.

In the following, we plot the 95\% confidence ellipse for the coefficients on `size` and `expenditure` from the last regression we conducted.
By specifying the additional argument `fill`, the confidence set colored which gives a better impression which set of coefficient pairs is meant.

<div class="unfolded">
```{r, echo=2:3, fig.align = 'center', warning=F, message=F}
model <- lm(score ~ size + english + expenditure, data = CASchools)
# Draw the 95% confidence set for coefficients on size and expenditure
confidenceEllipse(model, fill = T, which.coef = c("size","expenditure"))
```
</div>

We see that the ellipse is centered arrount $(-0.29, 3.87)$ i.e. the pair of coefficients estimates on `size` and `expenditure`.

### Model Specification for Multiple Regression

Choosing a regression specifaction i.e. selecting the variables to be included in a regression model can be quite cumbersome. However, there are some guidelines how to do this.
One could consider:
* Possible sources of omitted variable bias and control variables.
* Comparison of different regression specifications by means of $R^2$.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.3 </h3>          
<h3 class = "left"> Omitted Variable Bias in Multiple Regression</h3>

<p>

Omitted variable bias is the bias in the OLS estimator that arises when one or more included regressors are correlated with an omitted variable. For omitted variable bias to arise, two things must be true: 

1. At least one of the included regressors must be correlated with the omitted variable. 
2. The omitted variable must be a determinant of the dependent variable, $Y$.

</p>
  
</div>

We will now discuss an example were we face potential omitted variable bias in a multiple regression model:

Consider again the estimated regression model

$$ \widehat{TestScore} = \underset{(8.7)}{686.0} - \underset{(0.43)}{1.10} \times size - \underset{(0.031)}{0.650} \times english. $$

There might be a bias arising from omitting "outside learning opportunities" from our regression sice a measure like this could be a determinant of the students test scores and could also be correlated with both regressors already included in the model. Now, "outside learning opportunities" is a complicated concept that is difficult to quantify. A surrogate we can consider instead is the students' economic backgroud which should be strongly related the the outside learning opportunities. We thus augment the model with the variable `lunch`, the percentage of students that qualify for a free or subsidized lunch in school due to family incomes below a certain threshold, and estimate the model again.   

<div class="unfolded">
```{r, echo=6:7, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size  <- CASchools$students/CASchools$teachers
CASchools$score    <- (CASchools$read + CASchools$math)/2

model <- lm(score ~ size + english + lunch, data = CASchools)
summary(model)
```
</div>

Thus, the estimated regression line is

$$ \widehat{TestScore} = \underset{(4.7)}{700.2} - \underset{(0.24)}{1.00} \times size - \underset{(0.032)}{0.122} \times english + \underset{(0.022)}{0.547} \times lunch. $$

We observe no substantial changes in the conclusion about the effect of `size`: the coefficient changes by only $0.1$ and keeps its significance. 

Although the difference in estimated coefficients is not big in this case, it might be a good idea to keep  `lunch` in the model to make the assumption of conditional mean independence more credible.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.4 </h3>          
<h3 class = "left"> $R^2$ and $\overline{R^2}$: What They Tell You --- and What They Don't  </h3>

<p>

<b><i>The $R^2$ and $\overline{R^2}$ tell you</i></b> whether the regressors are good at predicting, or "explaining" the values of the independent variable in the sample of data at hand. if the $R^2$ (or $\overline{R^2}$) is nearly 1, then the regressors produce good prediction of the dependent variable in that sample, in the sense that the variance of OLS residuals is small compared to the variance of the dependent variable. If the $R^2$ (or $\overline{R^2}$) is nearly 0, the opposite is true.

<b><i>The $R^2$ and $\overline{R^2}$ do <u>not</u> tell you </i></b> whether:

1. An included variable is statistically significant. 
2. The regressors are truea cause of the movements in the dependent variable
3. There is omitted variable bias, or
4. You have chosen the most appropriate set of regressors.

</p>
  
</div>

(Kein Beispiel im Buch. Eigenes hinzuf√ºgen.)

