## Section 7.2 Hypothesis Tests and Confidence Intervals in Multiple Regression

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.1 </h3>          
<h3 class = "left"> Testing the hypothesis $\beta_j = \beta_{j,0}$ <br>
                    against the alternative $\beta_j \neq \beta_{j,0}$ </h3>

<p>
1. Compute the standard error of $\hat{\beta_j}$
2. Compute the t-statistic,
$$t = \frac{\hat{\beta}_j - \beta_{j,0}} {SE(\hat{\beta_j})}$$
3. Compute the p-value,
$$p\text{-value} = 2 \Phi(-|t^{act}|)$$

where $t^{act}$ is the value of the t-statistic actually computed. Reject the hypothesis at the 5\% significance level if the p-value is less than $0.05$ or, equivalently, if $|t^{act}| > 1.96$. The standard error and (typically) the t-statistic and p-value testing $\beta_j = 0$ are computed automativally by statistical software.  
</p>
  
</div>

It is easy to verify that principles of testing single hypothesis about the significance of coefficients in the multiple regression model are just as in in the simple regression model. 

You can easily see this by inspecting the summary of the regression model

$$ TestScore = \beta_0 - \beta_1 \times size - \beta_2 \times english + u $$

already discussed in chapter 6.3.

<div class="fold o">
```{r, echo=5:7, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size  <- CASchools$students/CASchools$teachers
CASchools$score    <- (CASchools$read + CASchools$math)/2

model <- lm(score ~ size + english, data = CASchools)
summary(model)
```
</div>

You may check this by computing the $t$-statistics or $p$-values by hand using the output above and R as a calculator!

Using the definition of the $p$-value for a two sided test, we can confirm the $p$-value for the intercept coeffiecient to be approximatley zero:

```{r, warning=F, message=F}
2*(1-pnorm(abs(92.566)))
```


<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.2 </h3>          
<h3 class = "left"> Confidence Intervals for a single coefficient in multipe regression </h3>

<p>
A $95\%$ two-sided confidence interval for coefficient $\beta_j$ is an interval that contains the true value of $\beta_j$ with a $95 \%$ probability; that is, it contains the true value of $\beta_j$ in $95 \%$ of all randomly drawn samples. Equivalently, it is the set of values of $\beta_j$ that cannot be rejected by a $5 \%$ two-sided hypothesis test. When the sample size is large, the $95 \%$ confidence interval for $\beta_j$ is
$$[\hat{\beta_j}- 1.96 \times SE(\hat{\beta}_j), \hat{\beta_j} + 1.96 \times SE(\hat{\beta_j})]$$
</p>
</div>

Let us take a look at the regression from section 6.3 again.

Computing individual confidence intervals for the coefficients in the multiple regression model can be done analogously as in the simple regression model using the function `confint`:

```{r, warning=F, message=F}
model <- lm(score ~ size + english, data = CASchools)
confint(model)
```

We note that intervals for all three coefficients are computed. 

If we want to compute confidence intervalls at another level of $\alpha$, 0.1 say, we have to set the argument `level` of the `confint` function accordingly.

```{r, warning=F, message=F}
confint(model, level = 0.9)
```

#### An Application to Test Scores and th Student-Teacher Ratio

Knowing how to use R to draw inference for multiple Regression models, you can now answer the following question: Can the null hypothesis that a change in the student-teacher ratio has no significant effect on test scores if we control for the percentage of students learning English in the district be rejected?
 
The outputs above tell us that 0 is not an element of the computed confidence intervals for the coefficient of `size` such that we can reject the null hypothesis at significance levels of 5\% and 10\% (Note that rejection at the 5\%-level implies rejection at the 10\% level. Why?). The same conclusion can be made when beholding the $p$-value for `size`: $0.00398 < 0.05 = \alpha$.

The 95\% confidence intervall tells us that we can be 95\% confident that a one-unit decrease in the student-teacher ratio has an effect on test scores that lies in the intervall $[-1.8487969,  -0.3537944]$

#### Another Augmentation of the Model

What is the effect on test scores of reducing the student-teacher ratio when the expenditures per pupil and the percentage of english learning pupils are held constant?

We can pursue this by augmenting our model equation yet again and perform joint significant testing.

The model we want the estimate now is 

$$ TestScore = \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times Expn + u $$

with $Expn$ the total amount of expenditures per pupil in the district (thousands of dollars).

Estimating the model equation yields:

```{r, echo=6:8, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size  <- CASchools$students/CASchools$teachers
CASchools$score    <- (CASchools$read + CASchools$math)/2

CASchools$expenditure <- CASchools$expenditure/1000
model <- lm(score ~ size + english + expenditure, data = CASchools)
summary(model)
```

We see that the estimated effect of a change in the student-teacher ratio on test scores with expenditures per pupil and the share of english learning pupils held constant is rather small (`-0.29`). What is more, the coefficient of `size` is not significantly different from zero anymore ($p\text{-value}=$`0.55) so the new model provides no evidence that changing the student-teacher ratio has any effect on the test scores in this case.

```{r, echo=5:7, warning=F, message=F}
model <- lm(score ~ size + english, data = CASchools)
summary(model)
```

#### Joint Hypothesis testing using the $F$-Statistic

Now, can we reject the hypothesis that $\beta_1$ \underline{and} $\beta_3$ are zero? To answer this question, we have to resort to methods that allow joint testing of two hypotheses. This is different from conducting individual $t$-tests.

The homoskedasticity-only $F$-Statistic is given by

$$ F = \frac{(SSR_{restricted} - SSR_{unrestricted})/q}{SSR_{unrestricted} / (n-k_{unrestricted}-1)} $$

with $SSR_{restricted}$ the sum of squared residuals from the restricted regression, i.e. the regression where we impose the restriction. $SSR_{unrestricted}$ is the sum of squared residuals from the full model, $q$ is the number of restriction under the null and $k$ is the number of regressors in the unrestricted regression.

Luckily, it is fairly easy to conduct an $F$-test in R. We can use the function `linearHypothesis` coming with the `car` package.

```{r, warning=F, message=F}
model <- lm(score ~ size + english + expenditure, data = CASchools)
# execute the function on the model object and provide the linear restriction to be tested as strings
linearHypothesis(model, c("size=0","expenditure=0"))
```

From the output we can infer that the $F$-statistic for this joint hypothesis test is around `-8.01` and the corresponding $p$-value is `0.000386`. Thus we can reject the null hypothesis that both coefficients are zero at any level of significance common in practice.

<b>Note</b>:

The standard output of a model summary also reports a related $p$-value corresponding to a $F$-test. The null hypothesis belonging to this quantity is that <u>all</u> population coefficients in the model except for the intercept are zero which is different from testing if only $\beta_1$ and $\beta_3$ are zero.

We will now check that the $F$-statistic belonging to the $p$-values listed in the model's summary coincides with the result reportet by `linearHypothesis`:

```{r, warning=F, message=F}
# execute the function on the model object and provide the linear restriction to be tested as strings
linearHypothesis(model, c("size=0","english=0","expenditure=0"))
# Access the F-statistic from the model's summary
summary(model)$fstatistic
```

#### Confidence Sets for Multiple Coefficients

Based on the $F$-statistic that we have previously encountered, we can specify confidence sets i.e. sets consisting of combinations of coefficients that contain the true combination of coefficients in, 95\% say, of all cases if we could infinitely draw random samples, just as in the univariate case. Put differently, it is the set of coefficient combinations for which we cannot reject the null hypothesis related to the computed $F$-statistic.  

If we consider two coefficients, their confidence set is an ellipse centered around the coefficient estimates. Again, there is a very convenient way to plot the "confidence ellipse" for coefficients of model objects, namely the function `ellipse` which is also coming with the `car` package.

In the following, we plot the 95\% confidence ellipse for the coefficients on `size` and `expenditure` from the last regression we conducted.
By specifying the additional argument `fill`, the confidence set colored which gives a better impression which set of coefficient pairs is meant.


```{r, echo2:3, warning=F, message=F}
model <- lm(score ~ size + english + expenditure, data = CASchools)
# Draw the 95% confidence set for coefficients on size and expenditure
confidenceEllipse(model, fill = T, which.coef = c("size","expenditure"))
```


We see that the ellipse is centered arrount $(-0.29, 3.87)$ i.e. the pair of coefficients estimates on `size` and `expenditure`.

#### Model Specification for Multiple Regression

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.3 </h3>          
<h3 class = "left"> Omitted variable bias in multiple regression</h3>

<p>

Omitted Variable bias is the bias in the OLS estimator that arises when one or more included regressors are correlated with an omitted variable. For omitted variable bias to arise, two things must be true: 

1. At least one of the included regressors must be correlated with the omitted variable. 
2. The omitted variable must be a determinant of the dependent variable, $Y$.

</p>
  
</div>

We will now discuss an example were we face potential omitted variable bias in a multiple regression model:

Consider again the estimated regression model

$$ \widehat{TestScore} = \underset{(8.7)}{686.0} - \underset{(0.43)}{1.10} \times size - \underset{(0.031)}{0.650} \times english. $$

There might be a bias arising from omitting "outside learning opportunities" from our regression sice a measure like this could be a determinant of the students test scores and could also be correlated with both regressors already included in the model. Now, "outside learning opportunities" is a complicated concept that is difficult to quantify. A surrogate we can consider instead is the students' economic backgroud which should be strongly related the the outside learning opportunities. We thus augment the model with the variable `lunch`, the percentage of students that qualify for a free or subsidized lunch in school due to family incomes below a certain threshold, and estimate the model again.   

<div class="fold o">
```{r, echo=6:7, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size  <- CASchools$students/CASchools$teachers
CASchools$score    <- (CASchools$read + CASchools$math)/2

model <- lm(score ~ size + english + lunch, data = CASchools)
summary(model)
```
</div>

Thus, the estimated regression line is

$$ \widehat{TestScore} = \underset{(4.7)}{700.2} - \underset{(0.24)}{1.00} \times size - \underset{(0.032)}{0.122} \times english + \underset{(0.022)}{0.547} \times lunch. $$

We observe no substantial changes in the conclusion about the effect of `size`: the coefficient changes by only $0.1$ and keeps its significance. 

Although the difference in estimated coefficients is not big in this case, it might be a good idea to keep  `lunch` in the model to make the assumption of conditional mean independence more credible.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 7.4 </h3>          
<h3 class = "left"> $R^2$ and $\overline{R^2}$: What They Tell You --- and What They Don't  </h3>

<p>

<b><i>The $R^2$ and $\overline{R^2}$ tell you</i></b> whether the regressors are good at predicting, or "explaining" the values of the independent variable in the sample of data at hand. if the $R^2$ (or $\overline{R^2}$) is nearly 1, then the regressors produce good prediction of the dependent variable in that sample, in the sense that the variance of OLS residuals is small compared to the variance of the dependent variable. If the $R^2$ (or $\overline{R^2}$) is nearly 0, the opposite is true.

<b><i>The $R^2$ and $\overline{R^2}$ do <u>not</u> tell you </i></b> whether:

1. An included variable is statistically significant. 
2. The regressors are truea cause of the movements in the dependent variable
3. There is omitted variable bias, or
4. You have chosen the most appropriate set of regressors.

</p>
  
</div>

