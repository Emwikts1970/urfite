<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/textmate.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="css\theme.min.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>

<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">URFITE</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="glyphicon glyphicon-book"></span>
     
    Chapters
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="index.html#chapter-3">Chapter 3</a>
    </li>
    <li>
      <a href="index.html#chapter-4">Chapter 4</a>
    </li>
    <li>
      <a href="index.html#chapter-5">Chapter 5</a>
    </li>
    <li>
      <a href="index.html#chapter-6">Chapter 6</a>
    </li>
    <li>
      <a href="index.html#chapter-7">Chapter 7</a>
    </li>
  </ul>
</li>
<li>
  <a href="index.html#about-urfite">
    <span class="glyphicon glyphicon-info-sign"></span>
     
    About
  </a>
</li>
<li>
  <a href="javascript:history.go(-1)">Back</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<pre><code>                            ---</code></pre>
<p>title: “Introduction to Econometrics with R” author: “M. Arnold, M. Schmelzer, A. Gerber” date: “2016-07-12” output: html_document —</p>
<!--Include script for hiding output chunks-->
<script src="js/hideOutput.js"></script>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Hier kÃ¶nnen wir was zum Projekt schreiben! Worum gehts, wie ist es aufgebaut …</p>
<p><br> <br> <br></p>
</div>
<div id="chapter-3" class="section level1">
<h1>Chapter 3</h1>
<div id="section-3.5" class="section level2">
<h2>Section 3.5</h2>
<div id="table-3.1---not-completed-sig.-test" class="section level3">
<h3>Table 3.1 - Not completed (sig. test)</h3>
<p>In order to reproduce the data from table 3.1 you need to download the data from the books source (LINK). We simply read in the data and then make use of the powerful package dplyr to subset the data.</p>
<div class="fold o">
<pre class="r"><code>library(dplyr, quietly=T)
# Read in CPS data (taken from http:// ....)
cps_dat &lt;- read.csv(&quot;data/cps_ch3.csv&quot;, sep=&quot;;&quot;, dec=&quot;.&quot;)
# Get an overview of the data structure
head(cps_dat)</code></pre>
<pre><code>##   a_sex year    ahe12
## 1     1 1992 18.30969
## 2     1 1992 16.36428
## 3     1 1992 24.47649
## 4     2 1992 14.17163
## 5     1 1992 23.60233
## 6     2 1992 12.98128</code></pre>
<pre class="r"><code># Take cps_dat %&gt;% group data by gender and year %&gt;% compute the mean, standard deviation
# and number of observations for each group
avgs &lt;- cps_dat %&gt;% group_by(a_sex, year) %&gt;% summarise(mean(ahe12), sd(ahe12), n())
print(avgs, digits=2)</code></pre>
<pre><code>## Source: local data frame [12 x 5]
## Groups: a_sex [?]
## 
##    a_sex  year mean(ahe12) sd(ahe12)   n()
##    (int) (int)       (dbl)     (dbl) (int)
## 1      1  1992    24.83019 10.852310  1594
## 2      1  1996    23.97028 10.794585  1380
## 3      1  2000    26.54988 12.376707  1303
## 4      1  2004    26.80202 12.811654  1894
## 5      1  2008    26.63023 12.573526  1839
## 6      1  2012    25.29886 12.091118  2004
## 7      2  1992    21.38683  8.394597  1368
## 8      2  1996    20.25632  8.486105  1230
## 9      2  2000    22.12856  9.986240  1181
## 10     2  2004    22.42996  9.989347  1735
## 11     2  2008    22.26640 10.300938  1871
## 12     2  2012    21.50238  9.986548  1951</code></pre>
</div>
<p>Now that we have the numbers we can compute the difference between both genders.</p>
<div class="fold o">
<pre class="r"><code>male   &lt;- avgs %&gt;% filter(a_sex == 1) 
female &lt;- avgs %&gt;% filter(a_sex == 2)
colnames(male)   &lt;- c(&quot;Sex&quot;, &quot;Year&quot;, &quot;Y_bar_m&quot;, &quot;s_m&quot;, &quot;n_m&quot;)
colnames(female) &lt;- c(&quot;Sex&quot;, &quot;Year&quot;, &quot;Y_bar_f&quot;, &quot;s_f&quot;, &quot;n_f&quot;)

# Gender gap, standard errors and confidence intervals
gap      &lt;- male$Y_bar_m - female$Y_bar_f
gap_se   &lt;- sqrt(male$s_m^2 / male$n_m + female$s_f^2 / female$n_f)
gap_ci_l &lt;- gap - 1.96 * gap_se
gap_ci_u &lt;- gap + 1.96 * gap_se
result &lt;- cbind(male[,-1], female[,-(1:2)], gap, gap_se, gap_ci_l, gap_ci_u)
print(result, digits = 2)</code></pre>
<pre><code>##   Year Y_bar_m s_m  n_m Y_bar_f  s_f  n_f gap gap_se gap_ci_l gap_ci_u
## 1 1992      25  11 1594      21  8.4 1368 3.4   0.35      2.7      4.1
## 2 1996      24  11 1380      20  8.5 1230 3.7   0.38      3.0      4.5
## 3 2000      27  12 1303      22 10.0 1181 4.4   0.45      3.5      5.3
## 4 2004      27  13 1894      22 10.0 1735 4.4   0.38      3.6      5.1
## 5 2008      27  13 1839      22 10.3 1871 4.4   0.38      3.6      5.1
## 6 2012      25  12 2004      22 10.0 1951 3.8   0.35      3.1      4.5</code></pre>
</div>
</div>
</div>
</div>
<div id="chapter-4" class="section level1">
<h1>Chapter 4</h1>
<div id="linear-regression-with-one-regressor" class="section level2">
<h2>Linear Regression with One Regressor</h2>
<p>This chapter introduces the simple linear regression model which relates one variable, <span class="math inline">\(X\)</span>, to another variable <span class="math inline">\(Y\)</span>. If for example a school cuts the class sizes by hiring new teachers, how would this effect the performance of the students? With linear regression we can not only examine whether the class size <span class="math inline">\(\left(X\right)\)</span> does have an impact on the test results <span class="math inline">\(\left(Y\right)\)</span>. We can also learn something about the direction and the strength of this effect.</p>
<p>To start with an easy example consider the following combinations of average test scores and the average student to teacher ratios in some districts:</p>
<pre class="r"><code>class_data &lt;- data.frame(TestScore           = c(680, 640, 670, 660, 630, 660, 635), 
                         StudentTeacherRatio = c(15, 17, 19, 20, 22, 23.5, 25)
                         )
class_data </code></pre>
<pre><code>##   TestScore StudentTeacherRatio
## 1       680                15.0
## 2       640                17.0
## 3       670                19.0
## 4       660                20.0
## 5       630                22.0
## 6       660                23.5
## 7       635                25.0</code></pre>
<p>If we use a simple linear regression model we assume that the relationship between the two variables can be represented by a straight line (<span class="math inline">\(y = mx + n\)</span>). Lets suppose that the “true” function which relates test scores and student to teacher ratio is</p>
<p><span class="math display">\[TestScore = 713 - 3 \times ClassSize.\]</span></p>
<p>Let us take a look at the data and the line from above.</p>
<pre class="r"><code>plot(TestScore ~ StudentTeacherRatio, data = class_data, ylim = c(600, 700), xlim = c(10,30))
abline(a = 713, b = -3)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We find that our line does not touch any of the points and still we argued that it represents the true relationship. The reason for this is the core problem (and also the right to exist) of statistics, randomness. There are almost always effects which cannot be explained in a deterministic fassion and which excaberate the finding of the “truth”.</p>
<p>In order to account for the differences the regression model is extended by an <strong>error term</strong> which covers these random effect. This <strong>error term</strong> generally accounts for all differences between the “true” regression line and the actual observed data. This could next to the pure randomness also be measerment errors or the effect of important but not considered independent variables.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.1
</h3>
<h3 class="left">
Terminology for the Linear Regression Model with a Single Regressor
</h3>
<p>
<p>The linear regression model is</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_1 + u_i \]</span></p>
<p>where</p>
<ul>
<li>the subscript i runs over observations, <span class="math inline">\(i = 1\)</span>, …, <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(Y_i\)</span> is the <em>dependent variable</em>, the <em>regressand</em>, or simply the <em>left-hand variable</em></li>
<li><span class="math inline">\(X_i\)</span> is the <em>independent variable</em>, the <em>regressor</em>, or simply the <em>left-hand variable</em></li>
<li><span class="math inline">\(\beta_0 + \beta_1 X\)</span> is <em>population the regression line</em> or the <em>population regression function</em></li>
<li><span class="math inline">\(\beta_0\)</span> is the <em>intercept</em> of the population regression line</li>
<li><span class="math inline">\(\beta_1\)</span> is the <em>slope</em> of the population regression line</li>
<li><span class="math inline">\(u_i\)</span> is the <em>error term</em></li>
</ul>
</p>
</div>
</div>
<div id="estimating-the-coefficients-of-the-linear-regression-model" class="section level2">
<h2>4.2 Estimating the Coefficients of the Linear Regression Model</h2>
<p>In practical situations the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> of the population regression line are unknown. Therefore, we must use data to estimate the two unknown parameters. We will demonstrate how to do that on a real data example. We want to relate test scores to class sizes in 420 California school restricts. The test score is the districtwide average of reading and math scores for fifth graders. Class size is measured as the number of students divided by the number of teachers (student-teacher ratio). The California Shool dataset is available in the <code>AER</code> package. After installing the package with <code>install.packages(AER)</code> and attaching it with <code>Library()</code> the dataset can be loaded using the <code>data()</code> function.</p>
<pre class="r"><code>library(AER)                                                    # contains the dataset 
data(CASchools) </code></pre>
<p>With <code>head()</code> we get a first overview over our data. This function shows only the first 6 rows of the dataset which prevents an overcrowded console output.</p>
<pre class="r"><code>head(CASchools)</code></pre>
<pre><code>##   district                          school  county grades students
## 1    75119              Sunol Glen Unified Alameda  KK-08      195
## 2    61499            Manzanita Elementary   Butte  KK-08      240
## 3    61549     Thermalito Union Elementary   Butte  KK-08     1550
## 4    61457 Golden Feather Union Elementary   Butte  KK-08      243
## 5    61523        Palermo Union Elementary   Butte  KK-08     1335
## 6    62042         Burrel Union Elementary  Fresno  KK-08      137
##   teachers calworks   lunch computer expenditure    income   english  read
## 1    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6
## 2    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5
## 3    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3
## 4    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9
## 5    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8
## 6     6.40  12.3188 86.9565       25    5580.147 10.415000 12.408759 605.7
##    math
## 1 690.0
## 2 661.9
## 3 650.9
## 4 643.5
## 5 639.9
## 6 605.4</code></pre>
<p>We find that the dataset consists of plenty variables but the two we are intersted in (average test scores and student-teacher ratio) are not included. However, it is easily possible to calculate both from the existing data.</p>
<pre class="r"><code>CASchools$tsratio  &lt;- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    &lt;- (CASchools$read + CASchools$math)/2       # average test-score</code></pre>
<p>The next code chunk reproduces table 4.1 from the text book which summatizes the distribution of test scores and class sizes. In order to have a nice display format we gather all data after the computation in a data frame.</p>
<div class="fold o">
<pre class="r"><code>avg_tsratio   &lt;- mean(CASchools$tsratio) 
avg_score     &lt;- mean(CASchools$score)
sd_tsratio    &lt;- sd(CASchools$tsratio) 
sd_tsratio    &lt;- sd(CASchools$score)
quantiles     &lt;- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_tsratio &lt;- quantile(CASchools$tsratio, quantiles)
quant_score   &lt;- quantile(CASchools$score, quantiles)

#Gather everything in a data frame 
DistributionSummary &lt;- data.frame(Average           = c(avg_tsratio, avg_tsratio), 
                                  StandardDeviation = c(sd_tsratio, sd_tsratio), 
                                  quantile          = rbind(quant_tsratio, quant_score)
                                  )
DistributionSummary</code></pre>
<pre><code>##                Average StandardDeviation quantile.10. quantile.25.
## quant_tsratio 19.64043          19.05335      17.3486     18.58236
## quant_score   19.64043          19.05335     630.3950    640.05000
##               quantile.40. quantile.50. quantile.60. quantile.75.
## quant_tsratio     19.26618     19.72321      20.0783     20.87181
## quant_score      649.06999    654.45000     659.4000    666.66249
##               quantile.90.
## quant_tsratio     21.86741
## quant_score      678.85999</code></pre>
</div>
<p>Figure 4.2 shows a scatterplot of all observations. We see that the points are scatterd strongly and the correlation is with <span class="math inline">\(-0.23\)</span> rather weak. The task we are facing now is to find the line which fit best to this data.</p>
<div class="fold o">
<pre class="r"><code>cor(CASchools$tsratio, CASchools$score)</code></pre>
<pre><code>## [1] -0.2263627</code></pre>
<pre class="r"><code>plot(score ~ tsratio, 
     data = CASchools,
     main = &quot;Scatterplot of Test Score vs. Student-Teacher Ratio&quot;, 
     xlab = &quot;Student teacher-ratio (X)&quot;,
     ylab = &quot;Test Score (Y)&quot;,
     xlim = c(10,30),
     ylim = c(600, 720))</code></pre>
<p><img src="index_files/figure-html/Figure%204.2-1.png" width="672" /></p>
</div>
</div>
<div id="the-ordinary-least-squares-estimator-ols" class="section level2">
<h2>The Ordinary Least Squares Estimator (OLS)</h2>
<p>The OLS estimator chooses the regression coefficients so that the estimated regression line is as close as possible to the observed data. Closeness is measured by the sum of the squared mistakes made in predicting <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Let <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> be some estimator of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> then the total squared estimation mistakes can be expressed as:</p>
<p><span class="math display">\[\sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2\]</span>.</p>
<p>The OLS Estimator is the pair of estimators for intercept and slope which minimizes this expression.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.2
</h3>
<h3 class="left">
The OLS Estimator, Predicted Values, and Residuals
</h3>
<p>
<p>The OLS estimators of the slope <span class="math inline">\(\beta_1\)</span> and the intercept <span class="math inline">\(\beta_0\)</span> are</p>
\begin{align}
\hat{\beta_1} &amp; =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2} = \frac{s_{xy}} {s^2_X}  \\
\\
\hat{\beta_0} &amp; =  \bar{Y} - \hat{\beta_1} \bar{X} 
\end{align}
<p>The OLS predicted values</p>
\begin{align}
\hat{Y_i} &amp; =  \hat{\beta_0} + \hat{\beta_1}X_i\\
\\
\\
\hat{u_i} &amp; =  Y_i - \hat{Y_i} 
\end{align}
<p>The estimated intercept <span class="math inline">\(\left(\hat{\beta_0}\right)\)</span>, slope <span class="math inline">\(\left(\hat{\beta_1}\right)\)</span>, and residuals <span class="math inline">\(\left(\hat{\u_i}\right)\)</span> are computed from a sample of n observations of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i\)</span>, <span class="math inline">\(...\)</span>, <span class="math inline">\(n\)</span>. These are estimates of</p>
</p>
</div>
<p>Let us apply the formulas above to the model … and add the regression line to our plot (figure 4.2).</p>
<pre class="r"><code>attach(CASchools)
beta_1 &lt;- sum((tsratio - mean(tsratio))*(score - mean(score))) / sum((tsratio - mean(tsratio))^2)
beta_0 &lt;- mean(score) - beta_1 * mean(tsratio)</code></pre>
<pre class="r"><code>abline(a = beta_0, b = beta_1) # add regression line</code></pre>
<div class="figure" style="text-align: center">
<img src="index_files/figure-html/unnamed-chunk-9-1.png" alt="Figure 4.3" width="672" />
<p class="caption">
Figure 4.3
</p>
</div>
<p>There is a function called <code>lm</code> (<strong>l</strong>inear <strong>m</strong>odel) that can compute the OLS for you. It also takes care of a lot of other things like the residuals, fitted values, etc. Use <code>lm</code> to compute the OLS and add them to the figure 4.2 as well to check whether the coefficients really are the same.</p>
<pre class="r"><code>linear_model &lt;- lm(score ~ tsratio, data = CASchools)
linear_model</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ tsratio, data = CASchools)
## 
## Coefficients:
## (Intercept)      tsratio  
##      698.93        -2.28</code></pre>
<pre class="r"><code>abline(linear_model, lty=2, col=&quot;red&quot;)</code></pre>
<div class="figure" style="text-align: center">
<img src="index_files/figure-html/Figure%204.3-1.png" alt="Figure 4.4" width="672" />
<p class="caption">
Figure 4.4
</p>
</div>
</div>
</div>
<div id="chapter-5" class="section level1">
<h1>Chapter 5</h1>
<p>In this chapter, we continue with the treatment of the simple linear regression model. We will see how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty. <br> This chapter covers:</p>
<ul>
<li><p>Testing Hypotheses about regression coefficients</p></li>
<li><p>Confidence intervals for regression coefficients</p></li>
<li><p>Regression when <span class="math inline">\(X\)</span> is a dummy variable</p></li>
<li><p>Heteroskedasticity and Homoskedasticity</p></li>
</ul>
<div id="section-5.1-two-sided-hypotheses-concerning-beta_1" class="section level2">
<h2>Section 5.1 Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></h2>
<p>Using the fact that <span class="math inline">\(\hat{\beta}_1\)</span> is approximately normal distributed in large samples, hypothesis testing about the true value <span class="math inline">\(\beta_1\)</span> can be done with the same approach as discussed in chapter 3.2.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.1
</h3>
<h3 class="left">
General Form of the <span class="math inline">\(t\)</span>-Statistic
</h3>
<p>Remember from section 3.2 that a general <span class="math inline">\(t\)</span>-statistic has the form</p>
<p><span class="math display">\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]</span></p>
</div>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.2
</h3>
<h3 class="left">
Testing Hyothesis about <span class="math inline">\(\beta_1\)</span>
</h3>
<p>For testing the hypothesis <span class="math inline">\(H_0: \beta_1 = \beta_{1,0}\)</span>, we need to perform the following steps:</p>
<ol style="list-style-type: decimal">
<li>Compute the standard error of <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(SE(\hat{\beta}_1)\)</span></li>
</ol>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Compute the <span class="math inline">\(t\)</span>-statistic.</li>
</ol>
<p><span class="math display">\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) } \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Reject at the <span class="math inline">\(5\%\)</span> level if <span class="math inline">\(|t^{act}| &gt; 1.96\)</span> or, equivalently, if the <span class="math inline">\(p\)</span>-value is less than 0.05. <span class="math display">\[
p \text{-value} = \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| &gt; \left| \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] = \text{Pr}_{H_0} (|t| &gt; |t^{act}|) = 2 \Phi(-|t^{act}|)
\]</span> The last equality holds due to the normal approximation.</li>
</ol>
</div>
<p>Consider again the previous OLS regression stored in <code>linear_model</code> that gave us the regression line <span class="math display">\[ \widehat{score} = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times tsratio.  \]</span> For testing a hypothesis regarding the slope parameter, we need <span class="math inline">\(SE(\hat{\beta}_1)\)</span>, the standard error of the respective point estimator. These are presented in parantheses under the respective point estimate.</p>
<p>How can we get these values using R? By looking at the second column of the coefficients’ summary, we discover both values. In the third column, we find <span class="math inline">\(t^{act}\)</span> for tests of hypotheses <span class="math inline">\(H_0: \beta_0=0\)</span> and <span class="math inline">\(H_0: \beta_1=0\)</span>. Furthermore we find corresponding <span class="math inline">\(p\)</span>-values in the fourth column.</p>
<pre class="r"><code>summary(linear_model)$coefficients</code></pre>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## tsratio      -2.279808  0.4798255 -4.751327  2.783308e-06</code></pre>
<p>Let’s have a closer look at the test <span class="math inline">\(H_0: \beta_1=0 \, v. \, H_1: \beta_1 \neq 0\)</span>. Using our revisited knowledge about <span class="math inline">\(t\)</span>-statistics we see that</p>
<p><span class="math display">\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75 \]</span></p>
<p>What does this tell us about the significance of the estimated coefficient? Using R we may visualise how such a statement can be made:</p>
<pre class="r"><code># Plot the standard normal density
z &lt;- seq(-6,6,0.01)
tact &lt;- -4.75
plot(z, dnorm(z,0,1), type = &quot;l&quot;, col=&quot;steelblue&quot;, lwd=2, yaxs=&quot;i&quot;, bty = &quot;n&quot;, axes=F, ylab = &quot;&quot;, cex.lab=0.7)
axis(1, at = c(0,-1.96,1.96,-tact,tact), cex.axis=0.7)

# Shade the critical regions
polygon(c(-6,seq(-6,-1.96,0.01),-1.96),c(0,dnorm(seq(-6,-1.96,0.01)),0),col=&#39;orange&#39;)
polygon(c(1.96,seq(1.96,6,0.01),6),c(0,dnorm(seq(1.96,6,0.01)),0),col=&#39;orange&#39;)

# Add arrows and text indicating critical regions and the p-value
arrows(-3.5,0.2,-2.5,0.02, length = 0.1)
arrows(3.5,0.2,2.5,0.02, length = 0.1)

arrows(-5,0.16,-4.75,0, length = 0.1)
arrows(5,0.16,4.75,0, length = 0.1)

text(-3.5,0.22, labels = &quot;p-value/2&quot;, cex = 0.7)
text(3.5,0.22, labels = &quot;p-value/2&quot;, cex = 0.7)

text(-5,0.18, labels = expression(t^{act}), cex = 0.7)
text(5,0.18, labels = expression(t^{act}), cex = 0.7)

# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96,1.96), ticksize  = 0.145, lwd = 2, col = &quot;darkred&quot;)
rug(c(-tact,tact), ticksize  = -0.0451, lwd = 2, col = &quot;darkgreen&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We reject the null hypothesis at the 5% level since <span class="math inline">\(|t^{act}| &gt; 1.96\)</span> or, alternatively and leading to the same result, <span class="math inline">\(p\text{-value} = 2.78*10^{-6} &gt; 0.05\)</span>. Hence, we conclude that the coefficient is significantly different from zero.</p>
</div>
<div id="section-5.2-confidence-intervals-for-regression-coefficients" class="section level2">
<h2>Section 5.2 Confidence Intervals for Regression Coefficients</h2>
<p>As we already know, estimates of regression coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have sampling uncertainty. Therefore, we will never estimate the exact true value of these parameters from sample data. However, we may construct confidence intervals for the intercept and the slope parameter.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.3
</h3>
<h3 class="left">
Confidence Interval for <span class="math inline">\(\beta_i\)</span>
</h3>
<p>Imagine you could draw all possible random samples of given size. The interval that contains the true value <span class="math inline">\(\beta_i\)</span> in <span class="math inline">\(95\%\)</span> of all samples is given by the expression</p>
<p><span class="math display">\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]</span></p>
<p>Equivalently, this interval can be seen as the set of null hypotheses for which a <span class="math inline">\(5\%\)</span> two-sided hypothesis test cannot reject.</p>
</div>
<p>Let us now turn back to the regression model stored in <code>linear_model</code>. An easy way to get <span class="math inline">\(95\%\)</span> confidence intervals for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is to use the function <code>confint()</code>. We only have to provide a fitted model object as an argument.</p>
<pre class="r"><code>confint(linear_model)</code></pre>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## tsratio      -3.22298  -1.336636</code></pre>
<p>Let us check if the calculation is done as we expect it to be. For <span class="math inline">\(\beta_1\)</span>, according to the formula presented above, interval borders are computed as</p>
<p><span class="math display">\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]</span></p>
<p>so this actually leads to the same interval. Obviously, this interval does not contain zero what, as we have already seen in section 5.1, leads to rejection of the null hypothesis <span class="math inline">\(\beta_{1,0} = 0\)</span>.</p>
</div>
<div id="section-5.3-regression-when-x-is-binary" class="section level2">
<h2>Section 5.3 Regression when X is Binary</h2>
<p>Instead of using a continuous regressor <span class="math inline">\(X\)</span>, we might be interested in running the regression</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 D_i + u_i \]</span></p>
<p>where <span class="math inline">\(D_i\)</span> is binary variable or so-called <em>dummy variable</em>. For example, we define <span class="math inline">\(D_i\)</span> in the following way:</p>
<p><span class="math display">\[ D_i = \begin{cases}
        1 \ \ \text{if $stratio$ in $i^{th}$ district &lt; 20} \\
        0 \ \ \text{if $stratio$ in $i^{th}$ district $\geq$ 20} \\
      \end{cases} \]</span></p>
<p>The regression model now is</p>
<p><span class="math display">\[ score_i = \beta_0 + \beta_1 D_i + u_i. \]</span></p>
<p>Let us see how these data points look like.</p>
<pre class="r"><code># Create the dummy variable as defined above
for (i in 1:nrow(CASchools)) {
  if (CASchools$tsratio[i] &lt; 20) { 
    CASchools$D[i] &lt;- 1
    } else {
      CASchools$D[i] &lt;- 0
    }
  }

# Plot the data
plot(CASchools$D, CASchools$score, 
     pch=20, cex=0.5 ,col=&quot;Steelblue&quot;,
     xlab=expression(D[i]), ylab=&quot;Test Score&quot;,
     main = &quot;Dummy Regression&quot;
     )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that it is not useful to think of <span class="math inline">\(\beta_1\)</span> as a slope parameter since <span class="math inline">\(D_i \in \{0,1\}\)</span>, i.e. we only observe two discrete values instead of continuous regressor values lying (in some range) on the real line. Simply put, there is no continuous line depicting the conditional expectation function <span class="math inline">\(E(score | D_i)\)</span> since this function is solely defined for positions <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>The interpretation of estimated coefficients from such a regression model is as follows:</p>
<ul>
<li><p><span class="math inline">\(E(Y_i | D_i = 0) = \beta_0\)</span> so <span class="math inline">\(\beta_0\)</span> is the expectation of <span class="math inline">\(score\)</span> in districts where <span class="math inline">\(D_i=0\)</span> i.e. where <span class="math inline">\(tsratio\)</span> is below <span class="math inline">\(20\)</span>.</p></li>
<li><p><span class="math inline">\(E(Y_i | D_i = 1) = \beta_0 + \beta_1\)</span> or, using the result above, <span class="math inline">\(\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\)</span>. Thus, <span class="math inline">\(\beta_1\)</span> is the difference in group specific expectations, i.e. the difference in expected <span class="math inline">\(score\)</span> between districts where <span class="math inline">\(tsratio &lt; 20\)</span> and those with <span class="math inline">\(tsratio \geq 20\)</span>.</p></li>
</ul>
<p>We will now use R to estimate the dummy regression.</p>
<pre class="r"><code>dummy_model &lt;- lm(score ~ D, data = CASchools)
summary(dummy_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ D, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  650.077      1.393 466.666  &lt; 2e-16 ***
## D              7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202</code></pre>
<p>One can see that the expected test score in districts with <span class="math inline">\(tsratio &lt; 20\)</span> (<span class="math inline">\(D_i = 1\)</span>) is predicted to be <span class="math inline">\(650.1 + 7.17 = 657.27\)</span> while districs with <span class="math inline">\(tsratio \geq 20\)</span> (<span class="math inline">\(D_i = 0\)</span>) are expected to have an average test score of only <span class="math inline">\(650.1\)</span>.</p>
<p>Group specific predictions can be added to the plot by execution of the following code chunk.</p>
<pre class="r"><code>points(CASchools$D, predict(dummy_model), col=&quot;red&quot;, pch=20)</code></pre>
<p>The red dots represent group averages in the sample. Accordingly, <span class="math inline">\(\hat{\beta}_1\)</span> can be seen as the difference in group averages.</p>
<p>By inspection of the output generated with <code>summary(dummy_model)</code> we may also find an awnser to the question wether there is a significant difference in group means. This can be assessed by a (two-tailed) test of the hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> for which the <span class="math inline">\(t\)</span>-statistic and the corresponding <span class="math inline">\(p\)</span>-value are computed defaultly by <code>summary()</code>.</p>
<p>Since <span class="math inline">\(t = 3.88 &gt; 1.96\)</span> we reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance. The same conclusion is drawn when using the <span class="math inline">\(p\)</span>-value indicating significance to the <span class="math inline">\(0.00012\%\)</span> level.</p>
<p>Equivalently, we may use the <code>confint()</code> function to compute a <span class="math inline">\(95\%\)</span> confidence interval for the true difference in means and see if the hypothesised value is element of this set.</p>
<pre class="r"><code>confint(dummy_model)</code></pre>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## D             3.539562  10.79931</code></pre>
<p>We reject the hypothesis at the <span class="math inline">\(5\%\)</span> significance level since <span class="math inline">\(\beta_{1,0} = 0\)</span> lies outside of the interval <span class="math inline">\([3.54, 10.8]\)</span>.</p>
</div>
<div id="section-5.4-heteroskedasticity-and-homoskedasticity" class="section level2">
<h2>Section 5.4 Heteroskedasticity and Homoskedasticity</h2>
<p>All inference made in previous subsections of Chapter 5 relies on the assumption that the error variance does not vary as regressor values change. But this will not necessarily be the case in practical applications.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.4
</h3>
<h3 class="left">
Heteroskedasticity and Homoskedasticity
</h3>
<ul>
<li><p>We say the error term of our regression model is homoskedastic if the variance of the conditional distribution of <span class="math inline">\(u_i\)</span> given <span class="math inline">\(X_i\)</span>, <span class="math inline">\(var(u_i|X=x)\)</span> is constant for all observations in our sample <span class="math display">\[ var(u_i|X=x) = \sigma^2 \ \forall \ i=1,\dots,n \]</span></p></li>
<li><p>If instead there is dependence of the conditional variance of <span class="math inline">\(u_i\)</span> on <span class="math inline">\(X_i\)</span>, the error term is said to be heteroskedastic. We then write <span class="math display">\[  var(u_i|X=x) = \sigma_i^2 \ , \ i=1,\dots,n \]</span></p></li>
<li><p>Homoskedasticity is a special case of heteroskedasticity</p></li>
</ul>
<p>For a better understanding of heteroskedasticity, we generate heteroskedastic data, estimate a linear regression model and then use boxplots for illustration of conditional distrubutions of residuals.</p>
<div class="fold o">
<pre class="r"><code>library(scales)

# Genrate some heteroskedastic data

set.seed(123) 
x &lt;- rep(c(10,15,20,25),each=25)
e &lt;- rnorm(100, sd=12)                
i &lt;- order(runif(100, max=dnorm(e, sd=12))) 
y &lt;- 720 - 3.3 * x + e[rev(i)]

# Plot data and inspect conditional distributions

mod &lt;- lm(y ~ x)
plot(x, y, main=&quot;An Example of Heteroskedasticity&quot;,
     xlab = &quot;Student teacher-ratio (X)&quot;,
     ylab = &quot;Test Score (Y)&quot;,
     cex=0.5, pch=19, xlim=c(8,27), ylim=c(600,710))
abline(mod, col=&quot;red&quot;)
boxplot(y~x, add=TRUE, at=c(10,15,20,25), col=alpha(&quot;gray&quot;, 0.4), border=&quot;black&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>In this example, it is straightforward to see that we face unequal conditional variances. Specifically, the variance of the test score (and therefore <span class="math inline">\(var(u_i|X=x)\)</span>) increases with <span class="math inline">\(X\)</span>.</p>
</div>
<div id="should-we-care-about-heteroskedasticity" class="section level3">
<h3>Should we care about heteroskedasticity?</h3>
<p>First, let us see how <span class="math inline">\(SE(\hat{\beta}_1)\)</span> is computed under the assumption of homoskedasticity. The <code>summary</code> function uses the homoskedasticity-only formula <span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \overset{\sim}{\sigma}^2_{beta_1} } = \sqrt{ \frac{SER^2}{\sum(X_i - \overline{X})^2} }. \]</span></p>
<p>This in fact is an estimator for the standard deviation of the estimator <span class="math inline">\(\hat{\beta}_1\)</span> that is inconsistent for the true value <span class="math inline">\(SE(\hat{\beta}_1)\)</span>. The implication is that <span class="math inline">\(t\)</span>-statistics computed in the manner of key concept 5.1 do not have a standard normal distribution, even in large samples. This issue may invalidate inference drawn from the previously treated tools for hypothesis testing.</p>
<p>We will now use R to compute the homoskedasticity-only standard error estimator for <span class="math inline">\(\hat{\beta}_1\)</span> by hand and see if it matches the value provided by <code>summary()</code>.</p>
<pre class="r"><code># Store model summary in &#39;mod&#39;
model &lt;- summary(linear_model)

# Extract the standard error of the regression from model summary
SER &lt;- model$sigma

# Compute the variation in &#39;tsratio&#39;
V &lt;- (nrow(CASchools)-1)*var(CASchools$tsratio)

# Compute the standard error of the slope parameter&#39;s estimator and print it
SE.beta_1.hat &lt;- sqrt(SER^2/V); SE.beta_1.hat</code></pre>
<pre><code>## [1] 0.4798255</code></pre>
<pre class="r"><code># Use logical operators to see if the value computed by hand matches the one provided 
# in mod$coefficients. Round estimates to 4 decimal places.

round(model$coefficients[2,2],4) == round(SE.beta_1.hat,4)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Indeed, the estimated values are equal.</p>
</div>
<div id="computation-of-heteroskedasticity-robust-standard-errors" class="section level3">
<h3>Computation of heteroskedasticity-robust standard errors</h3>
<p>Cosistent estimation of <span class="math inline">\(SE(\hat{\beta}_1)\)</span> under heteroskedasticity is granted when the following robust estimator is used.</p>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \]</span></p>
<p>Stadard error estimates computed this way are also referred to as Eicker-Huber-White standard errors.</p>
<p>It can be quite cumbersome to do so this calculation by hand. Luckily, there are R function for that purpose. A convenient one, named <code>vcovHC</code> is contained in the <code>sandwich</code> package.</p>
<p>Let us now compute robust standard error estimates for coefficients in <code>linear_model</code>.</p>
<pre class="r"><code>library(sandwich)

vcov &lt;- vcovHC(linear_model, type = &quot;HC0&quot;)
vcov</code></pre>
<pre><code>##             (Intercept)    tsratio
## (Intercept)  106.908469 -5.3383689
## tsratio       -5.338369  0.2685841</code></pre>
<p>The output is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix since these are the standard error estimates we seek:</p>
<pre class="r"><code>robust_se &lt;- sqrt(diag(vcov))
robust_se</code></pre>
<pre><code>## (Intercept)     tsratio 
##   10.339655    0.518251</code></pre>
<p>Now assume we want to generate a coefficient summary that reports robust standard error estimates for coefficient estimates, robust <span class="math inline">\(t\)</span>-statistics and corresponding <span class="math inline">\(p\)</span>-values for the regression model <code>linear_model</code>. This can be done as follows:</p>
<pre class="r"><code># We invoke the function `coeftest()` on our model. Further we specify in the argument `vcov.`
# that the Eicker-Huber-White estimator, &quot;HC0&quot;, is to be used.

coeftest(linear_model, vcov. = vcovHC(linear_model, type = &quot;HC0&quot;))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 698.93295   10.33966  67.597 &lt; 2.2e-16 ***
## tsratio      -2.27981    0.51825  -4.399 1.382e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see that values reported in the column ‘Std. Error’ equal the ones received using <code>sqrt(diag(vcov))</code>.</p>
</div>
</div>
</div>
<div id="chapter-6" class="section level1">
<h1>Chapter 6</h1>
<div id="section-6.3" class="section level2">
<h2>Section 6.3</h2>
<pre class="r"><code># Multiple Regression - Application to Test Scores and the Student-Teacher Ratio
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  &lt;- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    &lt;- (CASchools$read + CASchools$math)/2       # average test-score
model &lt;- lm(score ~ tsratio + english, data = CASchools)</code></pre>
</div>
<div id="section-6.4" class="section level2">
<h2>Section 6.4</h2>
<p>Taking the code from Section 6.3 you could simply use <code>summary(model)</code> to print the <span class="math inline">\(SER\)</span>, <span class="math inline">\(R^2\)</span> and adjusted-<span class="math inline">\(R^2\)</span>. You find them at the bottom of the output.</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ tsratio + english, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 686.03224    7.41131  92.566  &lt; 2e-16 ***
## tsratio      -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>You can also compute the parameters by hand using the formulas on p. 242.</p>
<pre class="r"><code>n &lt;- nrow(CASchools)    # number of observations (rows)
k &lt;- 2                  # 2 regressors
y_mean &lt;- mean(CASchools$score)

SSR &lt;- sum( residuals(model)^2 )
TSS &lt;- sum( ( CASchools$score - y_mean )^2 )
ESS &lt;- sum((fitted(model) - y_mean)^2)

SER &lt;- sqrt(1/(n-k-1) * SSR)
Rsq &lt;- 1 - (SSR / TSS)
adj_Rsq &lt;- 1 - (n-1)/(n-k-1) * SSR/TSS
cat(SER, Rsq, adj_Rsq, sep = &quot;    &quot;)</code></pre>
<pre><code>## 14.46448    0.4264315    0.4236805</code></pre>
</div>
<div id="section-6.7" class="section level2">
<h2>Section 6.7</h2>
<div id="examples-of-perfect-multicollinearity" class="section level3">
<h3>Examples of Perfect Multicollinearity</h3>
<p>If you use <code>lm</code> to estimate a model with a set of regressors that suffer from perfect multicollinearity the system will warn you (<em>1 not defined because of singularities</em>) and ignore the regressor(s) which is(are) assumed to be a linear combination of the others.</p>
<pre class="r"><code># Example 1
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  &lt;- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    &lt;- (CASchools$read + CASchools$math)/2       # average test-score
CASchools$FracEL    &lt;- CASchools$english/100
model &lt;- lm(score ~ tsratio + english + FracEL, data = CASchools)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ tsratio + english + FracEL, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients: (1 not defined because of singularities)
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 686.03224    7.41131  92.566  &lt; 2e-16 ***
## tsratio      -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  &lt; 2e-16 ***
## FracEL             NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>If you compute OLS by hand you will run into the problem as well but noone is helping you out. The computation simply fails. Why is this the case? Take the following example:</p>
<p>Assume you want to estimate a simple linear regression model with an intercept and one regressor: <span class="math display">\[ y_i = \beta_0 + \beta_1 x_i + u_i\]</span> When applying perfect multicollinearity <span class="math inline">\(x\)</span> has to be a linear combination of the other regressors. Since the only other regressor is a constant, <span class="math inline">\(x\)</span> has to be constant as well. When you recap the formula for <span class="math inline">\(\beta_1\)</span>, namely <span class="math display">\[ \hat{\beta_1} =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2}\]</span> you find the variance of the regressor <span class="math inline">\(x\)</span> in the nominator. Since the variance of a constant is zero, you are not able to compute this fraction. <span class="math inline">\(\hat{\beta}_1\)</span> remains undefined.</p>
<p><font style="color:#004c93; font-weight:bold;">Note:</font> In this special case the nominator equals zero as well. Can you show that?</p>
<pre class="r"><code># Example 2
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  &lt;- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    &lt;- (CASchools$read + CASchools$math)/2       # average test-score
CASchools$NVS      &lt;- ifelse(CASchools$tsratio &lt; 12, 0, 1)      # if tsratio smaller 12, NVS = 0, else NVS = 1
model &lt;- lm(score ~ tsratio + english + NVS, data = CASchools)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ tsratio + english + NVS, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients: (1 not defined because of singularities)
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 686.03224    7.41131  92.566  &lt; 2e-16 ***
## tsratio      -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  &lt; 2e-16 ***
## NVS                NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># Example 3
library(AER)                                                    # contains the dataset 
data(CASchools) 
CASchools$tsratio  &lt;- CASchools$students/CASchools$teachers  # teacher-student-ratio
CASchools$score    &lt;- (CASchools$read + CASchools$math)/2  # average test-score
CASchools$PctES    &lt;- 100 - CASchools$english  # Percentage of english speakers 
model &lt;- lm(score ~ tsratio + english + PctES, data = CASchools)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ tsratio + english + PctES, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients: (1 not defined because of singularities)
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 686.03224    7.41131  92.566  &lt; 2e-16 ***
## tsratio      -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  &lt; 2e-16 ***
## PctES              NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</div>
<div id="about-urfite" class="section level1">
<h1>About Urfite</h1>
<p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
