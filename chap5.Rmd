In this chapter, we continue with the treatment of the simple linear regression model. We will see how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty.
<br>
This chapter covers:

- Testing Hypotheses about regression coefficients

- Confidence intervals for regression coefficients

- Regression when $X$ is a dummy variable

- Heteroskedasticity and Homoskedasticity

### Testing Two-Sided Hypotheses Concerning $\beta_1$

Using the fact that $\hat{\beta}_1$ is approximately normal distributed in large samples, hypothesis testing about the true value $\beta_1$ can be done with the same approach as discussed in chapter 3.2.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.1 </h3>
<h3 class = "left"> General Form of the $t$-Statistic </h3>

Remember from section 3.2 that a general $t$-statistic has the form

\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]

</div>

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.2 </h3>
<h3 class = "left"> Testing Hyothesis about $\beta_1$ </h3>

For testing the hypothesis $H_0: \beta_1 = \beta_{1,0}$, we need to perform the following steps:

1. Compute the standard error of $\hat{\beta}_1$, $SE(\hat{\beta}_1)$

\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}
\]

2. Compute the $t$-statistic.

\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) } \]

3. Reject at the $5\%$ level if $|t^{act}| > 1.96$ or, equivalently, if the $p$-value is less than 0.05. \[
p \text{-value} = \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| > \left| \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] = \text{Pr}_{H_0} (|t| > |t^{act}|) = 2 \Phi(-|t^{act}|)
\] The last equality holds due to the normal approximation.
</div>

Consider again the previous OLS regression stored in `linear_model` that gave us the regression line
  \[ \widehat{score} = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times size.  \]
For testing a hypothesis regarding the slope parameter, we need $SE(\hat{\beta}_1)$, the standard error of the respective point estimator. These are presented in parantheses under the respective point estimate. 

How can we get these values using R? By looking at the second column of the coefficients' summary, we discover both values.
In the third column, we find $t^{act}$ for tests of hypotheses $H_0: \beta_0=0$ and $H_0: \beta_1=0$. Furthermore we find corresponding $p$-values in the fourth column.

<div class="unfolded">
```{r, echo=7}
library(AER)
data(CASchools)
CASchools$size     <- CASchools$students/CASchools$teachers     # class size
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
linear_model <- lm(score ~ size, data = CASchools)              # estimate the model

summary(linear_model)$coefficients
```
</div>

Let's have a closer look at the test $H_0: \beta_1=0 \, v. \, H_1: \beta_1 \neq 0$. Using our revisited knowledge about $t$-statistics we see that

\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75 \]

What does this tell about the significance of the estimated coefficient? 
Using R we may visualise how such a statement can be made:

<div class="unfolded">
```{r, fig.align='center'}
# Plot the standard normal density
z <- seq(-6,6,0.01)
tact <- -4.75
plot(z, dnorm(z,0,1), type = "l", col="steelblue", lwd=2, yaxs="i", bty = "n", axes=F, ylab = "", cex.lab=0.7)
axis(1, at = c(0,-1.96,1.96,-tact,tact), cex.axis=0.7)

# Shade the critical regions
polygon(c(-6,seq(-6,-1.96,0.01),-1.96),c(0,dnorm(seq(-6,-1.96,0.01)),0),col='orange')
polygon(c(1.96,seq(1.96,6,0.01),6),c(0,dnorm(seq(1.96,6,0.01)),0),col='orange')

# Add arrows and text indicating critical regions and the p-value
arrows(-3.5,0.2,-2.5,0.02, length = 0.1)
arrows(3.5,0.2,2.5,0.02, length = 0.1)

arrows(-5,0.16,-4.75,0, length = 0.1)
arrows(5,0.16,4.75,0, length = 0.1)

text(-3.5,0.22, labels = paste("0.025=",expression(alpha),"/2",sep = ""), cex = 0.7)
text(3.5,0.22, labels = paste("0.025=",expression(alpha),"/2",sep = ""), cex = 0.7)

text(-5,0.18, labels = expression(paste("-|",t[act],"|")), cex = 0.7)
text(5,0.18, labels = expression(paste("|",t[act],"|")), cex = 0.7)

# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96,1.96), ticksize  = 0.145, lwd = 2, col = "darkred")
rug(c(-tact,tact), ticksize  = -0.0451, lwd = 2, col = "darkgreen")

```
</div>

We reject the null hypothesis at the 5\% level since $|t^{act}| > 1.96$ or, alternatively and leading to the same result, $p\text{-value} = 2.78*10^{-6} > 0.05$. Hence, we conclude that the coefficient is significantly different from zero. 

### Confidence Intervals for Regression Coefficients

As we already know, estimates of regression coefficients $\beta_0$ and $\beta_1$ have sampling uncertainty. Therefore, we will never estimate the exact true value of these parameters from sample data. However, we may construct confidence intervals for the intercept and the slope parameter.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.3 </h3>
<h3 class = "left"> Confidence Interval for $\beta_i$ </h3>

Imagine You could draw all possible random samples of given size. The interval that contains the true value $\beta_i$ in $95\%$ of all samples is given by the expression

\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]

Equivalently, this interval can be seen as the set of null hypotheses for which a $5\%$ two-sided hypothesis test cannot reject.
</div>

Let us now turn back to the regression model stored in `linear_model`. An easy way to get $95\%$ confidence intervals for $\beta_0$ and $\beta_1$, the coefficients of `(intercept)` and `size`, is to use the function `confint()`. We only have to provide a fitted model object as an argument to this function. 

<div class="unfolded">
```{r}
confint(linear_model)
```
</div>

Let us check if the calculation is done as we expect it to be. For $\beta_1$, that is the coefficient on `size`, according to the formula presented above interval borders are computed as

\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]

so this actually leads to the same interval. Obviously, this interval does not contain the value zero what, as we have already seen in the previous section, leads to rejection of the null hypothesis $\beta_{1,0} = 0$.

### Regression when X is Binary

Instead of using a continuous regressor $X$, we might be interested in running the regression 

\[ Y_i = \beta_0 + \beta_1 D_i + u_i \]

where $D_i$ is binary variable or so-called *dummy variable*. 
For example, we define $D_i$ in the following way:

\[ D_i = \begin{cases}
        1 \ \ \text{if $size$ in $i^{th}$ district < 20} \\
        0 \ \ \text{if $size$ in $i^{th}$ district $\geq$ 20} \\
      \end{cases} \]

The regression model now is

\[ score_i = \beta_0 + \beta_1 D_i + u_i. \]

Let us see how these data points look like.

<div class="unfolded">
```{r, eval=F}
# Create the dummy variable as defined above using a for loop
for (i in 1:nrow(CASchools)) {
  if (CASchools$size[i] < 20) { 
    CASchools$D[i] <- 1
    } else {
      CASchools$D[i] <- 0
    }
  }

# Plot the data
plot(CASchools$D, CASchools$score,          # provide the data to be ploted
     pch=20,                                # use filled circles as plot symbols
     cex=0.5,                               # set size of plot symbols to 0.5
     col="Steelblue",                       # set the symbols' color to "Steelblue"
     xlab=expression(D[i]),                 # Set title and name of the axis
     ylab="Test Score",
     main = "Dummy Regression"
     )
```
</div>

<div class="unfolded">
```{r, fig.align='center', echo=F}
# Create the dummy variable as defined above
for (i in 1:nrow(CASchools)) {
  if (CASchools$size[i] < 20) { 
    CASchools$D[i] <- 1
    } else {
      CASchools$D[i] <- 0
    }
  }
dummy_model <- lm(score ~ D, data = CASchools)
# Plot the data
plot(CASchools$D, CASchools$score, 
     pch=20, cex=0.5 ,col="Steelblue",
     xlab=expression(D[i]), ylab="Test Score",
     main = "Dummy Regression"
     )
points(CASchools$D, predict(dummy_model), col="red", pch=20)
```
</div>

We see that it is not useful to think of $\beta_1$ as a slope parameter since $D_i \in \{0,1\}$, i.e. we only observe two discrete values instead of continuous regressor values lying (in some range) on the real line. Simply put, there is no continuous line depicting the conditional expectation function $E(score | D_i)$ since this function is solely defined for positions $0$ and $1$.

The interpretation of estimated coefficients from such a regression model is as follows:

- $E(Y_i | D_i = 0) = \beta_0$ so $\beta_0$ is the expectation of $score$ in districts where $D_i=0$ i.e. where $size$ is below $20$.

- $E(Y_i | D_i = 1) = \beta_0 + \beta_1$ or, using the result above, $\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)$. Thus, $\beta_1$ is the difference in group specific expectations, i.e. the difference in expected $score$ between districts where $size < 20$ and those with $size \geq 20$. 

We will now use R to estimate the dummy regression model.

<div class="unfolded">
```{r}
dummy_model <- lm(score ~ D, data = CASchools)
summary(dummy_model)
```
</div>

One can see that the expected test score in districts with $size < 20$ ($D_i = 1$) is predicted to be $650.1 + 7.17 = 657.27$ while districs with $size \geq 20$ ($D_i = 0$) are expected to have an average test score of only $650.1$.

Group specific predictions can be added to the plot by execution of the following code chunk.
```{r, eval=F}
points(CASchools$D, predict(dummy_model), col="red", pch=20)
```
The red dots represent group averages in the sample. Accordingly, $\hat{\beta}_1$ can be seen as the difference in group averages.

By inspection of the output generated with `summary(dummy_model)` we may also find an awnser to the question wether there is a significant difference in group means. This can be assessed by a (two-tailed) test of the hypothesis $H_0: \beta_1 = 0$ for which the $t$-statistic and the corresponding $p$-value are computed defaultly by `summary()`.

Since $t = 3.88 > 1.96$ we reject the null hypothesis at the $5\%$ level of significance. The same conclusion is drawn when using the $p$-value indicating significance to the $0.00012\%$ level. 

Equivalently, we may use the `confint()` function to compute a $95\%$ confidence interval for the true difference in means and see if the hypothesised value is element of this set. 

<div class="unfolded">
```{r}
confint(dummy_model)
```
</div>

We reject the hypothesis at the $5\%$ significance level since $\beta_{1,0} = 0$ lies outside of the interval $[3.54, 10.8]$.

### Heteroskedasticity and Homoskedasticity

All inference made in previous subsections of Chapter 5 relies on the assumption that the error variance does not vary as regressor values change. But this will not necessarily be the case in practical applications.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.4 </h3>          
<h3 class = "left"> Heteroskedasticity and Homoskedasticity </h3>

- We say the error term of our regression model is homoskedastic if the variance of the conditional distribution of $u_i$ given $X_i$, $var(u_i|X=x)$ is constant for all observations in our sample
\[ var(u_i|X=x) = \sigma^2 \ \forall \ i=1,\dots,n \]

- If instead there is dependence of the conditional variance of $u_i$ on $X_i$, the error term is said to be heteroskedastic. We then write
\[  var(u_i|X=x) = \sigma_i^2 \ , \ i=1,\dots,n \]

- Homoskedasticity is a special case of heteroskedasticity
</div>

For a better understanding of heteroskedasticity, we generate heteroskedastic data, estimate a linear regression model and then use boxplots for illustration of conditional distrubutions of residuals.

<div class="unfolded">
```{r, fig.align='center', warning=FALSE}
library(scales)

# Genrate some heteroskedastic data

set.seed(123) 
x <- rep(c(10,15,20,25),each=25)
e <- rnorm(100, sd=12)                
i <- order(runif(100, max=dnorm(e, sd=12))) 
y <- 720 - 3.3 * x + e[rev(i)]

# Estimate the model 
mod <- lm(y ~ x)

# Plot the data

plot(x, y, main="An Example of Heteroskedasticity",
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     cex=0.5, pch=19, xlim=c(8,27), ylim=c(600,710))

# Add the regression line to the plot
abline(mod, col="red")

# Add boxplots to the plot
boxplot(y~x, add=TRUE, at=c(10,15,20,25), col=alpha("gray", 0.4), border="black")
```
</div>
In this example, it is straightforward to see that we face unequal conditional variances. Specifically, the variance of the test score (and therefore $var(u_i|X=x)$) increases with $X$.

### Should We Care About Heteroskedasticity?

First, let us see how $SE(\hat{\beta}_1)$ is computed under the assumption of homoskedasticity. The `summary` function uses the homoskedasticity-only formula 
  \[ SE(\hat{\beta}_1) = \sqrt{ \overset{\sim}{\sigma}^2_{beta_1} } = \sqrt{ \frac{SER^2}{\sum(X_i - \overline{X})^2} }. \]

This in fact is an estimator for the standard deviation of the estimator $\hat{\beta}_1$ that is inconsistent for the true value $SE(\hat{\beta}_1)$ when there is heteroskedasticity. The implication is that $t$-statistics computed in the manner of key concept 5.1 do not have a standard normal distribution, even in large samples. This issue may invalidate inference drawn from the previously treated tools for hypothesis testing.

We will now use R to compute the homoskedasticity-only standard error estimator for $\hat{\beta}_1$ by hand and see if it matches the value provided by `summary()`.

<div class="unfolded">
```{r}
# Store model summary in 'mod'
model <- summary(linear_model)

# Extract the standard error of the regression from model summary
SER <- model$sigma

# Compute the variation in 'size'
V <- (nrow(CASchools)-1)*var(CASchools$size)

# Compute the standard error of the slope parameter's estimator and print it
SE.beta_1.hat <- sqrt(SER^2/V); SE.beta_1.hat

# Use logical operators to see if the value computed by hand matches the one provided 
# in mod$coefficients. Round estimates to four decimal places.

round(model$coefficients[2,2],4) == round(SE.beta_1.hat,4)

```
</div>

Indeed, the estimated values are equal.

### Computation of Heteroskedasticity-Robust Standard Errors

Cosistent estimation of $SE(\hat{\beta}_1)$ under heteroskedasticity is granted when the following robust estimator is used.

\[ SE(\hat{\beta}_1) = \sqrt{ \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \]

Stadard error estimates computed this way are also referred to as Eicker-Huber-White standard errors. 
It can be quite cumbersome to do so this calculation by hand. Luckily, there are R function for that purpose. A convenient one, named `vcovHC` is contained in the `sandwich` package. 

Let us now compute robust standard error estimates for coefficients in `linear_model`.

<div class="unfolded">
```{r}
library(sandwich)

vcov <- vcovHC(linear_model, type = "HC0")
vcov
```
</div>

The output is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix since these are the standard error estimates we seek:

<div class="unfolded">
```{r}
robust_se <- sqrt(diag(vcov))
robust_se
```
</div>

Now assume we want to generate a coefficient summary that reports robust standard error estimates for coefficient estimates, robust $t$-statistics and corresponding $p$-values for the regression model `linear_model`. This can be done as follows:

<div class="unfolded">
```{r}
# We invoke the function `coeftest()` on our model. Further we specify in the argument `vcov.`
# that the Eicker-Huber-White estimator "HC0" is used.

coeftest(linear_model, vcov. = vcovHC(linear_model, type = "HC0"))
```
</div>

We see that values reported in the column `Std. Error equal the ones received using `sqrt(diag(vcov))`.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 5.5 </h3>          
<h3 class = "left"> The Gauss-Markov-Theorem for $\hat{\beta}_1$ </h3>

Suppose the Assumptions made in Key Concept 4.3 hold *and* that the errors are *homoskedastic*. The OLS estimator is the best (in the sense of smallest variance) linear conditionally unbiased estimator (*BLUE*) in this setting.

Let us have a closer look at what this means:

- Estimators of $\beta_1$ that are linear functions of the $Y_1, \dots, Y_n$ and that are unbiased conditionally on the regressor $X_1, \dots, X_n$ can be written as \[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \] where the $a_i$ are weights that are allowed to depend on the $X_i$ but *not* on the $Y_i$. 

- We already know that $\overset{\sim}{\beta}_1$ has a sampling distribution. If now \[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1 \] we say that $\overset{\sim}{\beta}_1$ is a linear unbiased estimator of $\beta_1$, conditionally on the $X_1, \dots, X_n$.

- We may ask if $\overset{\sim}{\beta}_1$ is also the *best* estimator in this class, i.e. the most efficient one where "most efficient" means smallest variance. The weights $a_i$ play an important role here.
</div>

#### An Example to Clarify the Gauss-Markov Theorem

Consider the case of a regression of $Y_i,\dots,Y_n$ on a constant. Here, the $Y_i$ are assumed to be a random sample from a population with mean $\mu$ and variance $\sigma^2$. We know that the OLS estimator is the sample mean: \[ \hat{\beta}_1 = \overline{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i \]

Clearly, each observation is weighted by $a_i = \frac{1}{n}$.

We also know that $Var(\hat{\beta}_1)=\frac{\sigma^2}{n}$.

We will now use R for a simulation study to illustrate what happens to the variance of the estimator for $\beta_1$ if different weights \[ w_i = \frac{1 \pm \epsilon}{n} \] are assigned to either half of the sample $Y_1, \dots, Y_n$ instead of using  the weights implied by OLS , $\frac{1}{n}$.

<div class="unfolded">
```{r, fig.align='center'}

# Set sample size and number of repititions

n <- 100      
reps <- 1e5

# Choose epsilon and create a vector of weights as defined above

epsilon <- 0.8
w <- c( rep((1+epsilon)/n,n/2), rep((1-epsilon)/n,n/2) )

# Draw a random sample y_1,...,y_N from the standard normal distribution, 
# use both estimators 1e5 times and store the result in vectors ols and 
# weightedestimator

ols <- rep(NA,reps)
weightedestimator <- rep(NA,reps)

for (i in 1:reps)
{
  y <- rnorm(n)
  ols[i] <- mean(y)
  weightedestimator[i] <- crossprod(w,y)
}

# Plot kernel density estimates of the estimators distribution 

# OLS
plot(density(ols), col="purple", lwd=3, main="Density of OLS and Weighted Estimator", xlab="Estimates")

# Weighted
lines(density(weightedestimator), col="steelblue", lwd=3) 

# Add a dashed line at 0 and a legend
abline(v=0, lty=2)
legend('topright', c("OLS","Weighted"), col=c("purple","steelblue"), lwd=3)
```
</div>

What conclusion can we draw from the result?

- Both estimators seem to be unbiased: The means of their estimated distributions are zero.
- The `weightedestimator` is less efficient than the `ols` estimator: There is higher dispersion when weights are $w_i = \frac{1 \pm \epsilon}{n}$ instead of $w_i=\frac{1}{n}$. 

Hence, our simulation results confirm what is stated by the Gauss-Markov Theorem.

