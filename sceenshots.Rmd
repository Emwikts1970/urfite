---
title: "Introduction to Econometrics with R"
author: "M. Arnold, M. Schmelzer, A. Gerber"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

<!--Include script for hiding output chunks-->
<script src="js/hideOutput.js"></script>

# Introduction

Hier können wir was zum Projekt schreiben! Worum gehts, wie ist es aufgebaut ...

<br>
<br>
<br>

# Chapter 3
## Section 3.5

### Table 3.1 - Not completed (sig. test)
In order to reproduce the data from table 3.1 you need to download the data from the books source (LINK).
We simply read in the data and then make use of the powerful package dplyr to subset the data.

<div class="fold o">
```{r, echo=T, eval=T}
library(dplyr, quietly=T)
# Read in CPS data (taken from http:// ....)
cps_dat <- read.csv("data/cps_ch3.csv", sep=";", dec=".")
# Get an overview of the data structure
head(cps_dat)
# Take cps_dat %>% group data by gender and year %>% compute the mean, standard deviation
# and number of observations for each group
avgs <- cps_dat %>% group_by(a_sex, year) %>% summarise(mean(ahe12), sd(ahe12), n())
print(avgs, digits=2)
```
</div>

Now that we have the numbers we can compute the difference between both genders.

<div class="fold o">
```{r, echo=T}
male   <- avgs %>% filter(a_sex == 1) 
female <- avgs %>% filter(a_sex == 2)
colnames(male)   <- c("Sex", "Year", "Y_bar_m", "s_m", "n_m")
colnames(female) <- c("Sex", "Year", "Y_bar_f", "s_f", "n_f")

# Gender gap, standard errors and confidence intervals
gap      <- male$Y_bar_m - female$Y_bar_f
gap_se   <- sqrt(male$s_m^2 / male$n_m + female$s_f^2 / female$n_f)
gap_ci_l <- gap - 1.96 * gap_se
gap_ci_u <- gap + 1.96 * gap_se
result <- cbind(male[,-1], female[,-(1:2)], gap, gap_se, gap_ci_l, gap_ci_u)
print(result, digits = 2)
```
</div>

# Chapter 4
## Linear Regression with One Regressor 

This chapter introduces the simple linear regression model which relates one variable, $X$, to another variable $Y$. If for example a school cuts the class sizes by hiring new teachers, how would this effect the performance of the students? With linear regression we can not only examine whether the class size $\left(X\right)$ does have an impact on the test results $\left(Y\right)$. We can also learn something about the direction and the strength of this effect. 

To start with an easy example consider the following combinations of average test scores and the average student to teacher ratios in some districts:

```{r}
class_data <- data.frame(TestScore           = c(680, 640, 670, 660, 630, 660, 635), 
                         StudentTeacherRatio = c(15, 17, 19, 20, 22, 23.5, 25)
                         )
class_data 
```


If we use a simple linear regression model we assume that the relationship between the two variables can be represented by a straight line ($y = mx + n$). Lets suppose that the "true" function which relates test scores and student to teacher ratio is

$$TestScore = 713 - 3 \times ClassSize.$$

Let us take a look at the data and the line from above. 

```{r}
plot(TestScore ~ StudentTeacherRatio, data = class_data, ylim = c(600, 700), xlim = c(10,30))
abline(a = 713, b = -3)
```

We find that our line does not touch any of the points and still we argued that it represents the true relationship. The reason for this is the core problem (and also the right to exist) of statistics, randomness. There are almost always effects which cannot be explained in a deterministic fassion and which excaberate the finding of the "truth". 

In order to account for the differences the regression model is extended by an **error term** which covers these random effect. This **error term** generally accounts for all differences between the "true" regression line and the actual observed data. This could next to the pure randomness also be measerment errors or the effect of important but not considered independent variables.   


<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.1 </h3>          
<h3 class = "left"> Terminology for the Linear Regression Model with a Single Regressor </h3>

<p> The linear regression model is 

$$Y_i = \beta_0 + \beta_1 X_1 + u_i $$

where

- the subscript i runs over observations, $i = 1$, ..., $n$
- $Y_i$ is the *dependent variable*, the *regressand*, or simply the *left-hand variable*
- $X_i$ is the *independent variable*, the *regressor*, or simply the *left-hand variable*
- $\beta_0 + \beta_1 X$ is *population the regression line* or the *population regression function*
- $\beta_0$ is the *intercept* of the population regression line
- $\beta_1$ is the *slope* of the population regression line
- $u_i$ is the *error term*
</p>
</div>

##4.2 Estimating the Coefficients of the Linear Regression Model

In practical situations the intercept $\beta_0$ and slope $\beta_1$ of the population regression line are unknown. Therefore, we must use data to estimate the two unknown parameters. We will demonstrate how to do that on a real data example. We want to relate test scores to class sizes in 420 California school restricts. The test score is the districtwide average of reading and math scores for fifth graders. Class size is measured as the number of students divided by the number of teachers (student-teacher ratio). The California Shool dataset is available in the `AER` package. After installing the package with `install.packages(AER)` and attaching it with `Library()` the dataset can be loaded using the `data()` function. 

```{r}
library(AER)                                                    # contains the dataset 
data(CASchools) 
```

With `head()` we get a first overview over our data. This function shows only the first 6 rows of the dataset which prevents an overcrowded console output. 

```{r}
head(CASchools)
```

We find that the dataset consists of plenty variables but the two we are intersted in (average test scores and student-teacher ratio) are not included. However, it is easily possible to calculate both from the existing data. 

```{r}
CASchools$tsratio  <- CASchools$students/CASchools$teachers     # teacher-student-ratio
CASchools$score    <- (CASchools$read + CASchools$math)/2       # average test-score
```

The next code chunk reproduces table 4.1 from the text book which summatizes the distribution of test scores and class sizes. In order to have a nice display format we gather all data after the computation in a data frame. 

<div class="fold o">

```{r Table 4.1, results='hold'}
avg_tsratio   <- mean(CASchools$tsratio) 
avg_score     <- mean(CASchools$score)
sd_tsratio    <- sd(CASchools$tsratio) 
sd_tsratio    <- sd(CASchools$score)
quantiles     <- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_tsratio <- quantile(CASchools$tsratio, quantiles)
quant_score   <- quantile(CASchools$score, quantiles)

#Gather everything in a data frame 
DistributionSummary <- data.frame(Average           = c(avg_tsratio, avg_tsratio), 
                                  StandardDeviation = c(sd_tsratio, sd_tsratio), 
                                  quantile          = rbind(quant_tsratio, quant_score)
                                  )
DistributionSummary
```
</div>

Figure 4.2 shows a scatterplot of all observations. We see that the points are scatterd strongly and the correlation is with $-0.23$ rather weak. The task we are facing now is to find the line which fit best to this data. 

<div class="fold o">
```{r Figure 4.2}
cor(CASchools$tsratio, CASchools$score)
plot(score ~ tsratio, 
     data = CASchools,
     main = "Scatterplot of Test Score vs. Student-Teacher Ratio", 
     xlab = "Student teacher-ratio (X)",
     ylab = "Test Score (Y)",
     xlim = c(10,30),
     ylim = c(600, 720))
```
</div>

## The Ordinary Least Squares Estimator (OLS)

The OLS estimator chooses the regression coefficients so that the estimated regression line is as close as possible to the observed data. Closeness is measured by the sum of the squared mistakes made in predicting $Y$ given $X$. Let $b_0$ and $b_1$ be some estimator of $\beta_0$ and $\beta_1$ then the total squared estimation mistakes can be expressed as: 

$$\sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2$$.

The OLS Estimator is the pair of estimators for intercept and slope which minimizes this expression. 

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.2 </h3>          
<h3 class = "left"> The OLS Estimator, Predicted Values, and Residuals </h3>

<p> The OLS estimators of the slope $\beta_1$ and the intercept $\beta_0$ are

\begin{align}
\hat{\beta_1} & =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2} = \frac{s_{xy}} {s^2_X}  \\
\\
\hat{\beta_0} & =  \bar{Y} - \hat{\beta_1} \bar{X} 
\end{align}


The OLS predicted values $\hat{Y_i}$ and residuals $\hat{u_i}$ are

\begin{align}
\hat{Y_i} & =  \hat{\beta_0} + \hat{\beta_1}X_i\\
\\
\\
\hat{u_i} & =  Y_i - \hat{Y_i} 
\end{align}

The estimated intercept $\left(\hat{\beta_0}\right)$, slope $\left(\hat{\beta_1}\right)$, and residuals $\left(\hat{u_i}\right)$ are computed from a sample of n observations of $X_i$ and $Y_i$, $i$, $...$,  $n$. These are estimates of the unkown true population intercept $\left(\beta_0 \right)$, slope $\left(\beta_1\right)$, and error term $(u_i)$.
</p>
</div>

OLS is one of the most widely-used estimation techniques. Hence R, as a statistical programming language, already contains a built-in function called `lm` (**l**inear **m**odel) which can be used to carry out regression analyses. The first argument of the function is the formula with the basic syntax `y ~ x` where `y` is the dependent and `x` the explanatory variable. The argument `data` specifies the dataset to be used in the regression. We now revisit the example from the book where the relationship between students‘ test scores and the student-teacher ratio is analyzed. The following code uses `lm` to replicate equation $(4.11)$.

<div class="fold o">
```{r}
linear_model <- lm(score ~ tsratio, data = CASchools)
linear_model
```
</div>

#4.5 The Sampling Distribution of the OLS Estimator 

Because the OLS estimators $\hat{\beta_0}$ and $\hat{\beta_1}$ are computed from a randomly drawn sample, the estimators themselves are random variables with a probability distribution -the sampling distribution- that describes the values they could take over different random samples. Although the sampling distribution of $\hat{\beta_0}$ and $\hat{\beta_1}$ can be complicated when the sample size is small, it is possible to make certain statements about is that hold for all n. In particular 
$$ E(\hat{\beta_0}) = \beta_0 \text{ and } E(\hat{\beta_1}) = \beta_1,$$
that is, $\hat{\beta_0}$ and $\hat{\beta_1}$ are unbiased estimators of $\beta_0$ and $\beta_1$. If the sample is sufficiently large, by the central limit theorem zje sampling distribiution of the estimators is well approximated by the bivariate normal distribution. This implies that the marginal distributions are normal in large samples.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 4.4 </h3>          
<h3 class = "left"> Large Sample Distribution of $\hat{\beta_0}$ and $\hat{\beta_1}$ </h3>

<p> If the least square saaumption in Key Concept 4.3 hold, then in large-samples $\hat{\beta_0}$ and $\hat{\beta_1}$ have a jointly normal sampling distribution. The large-sample normal distribution of $\hat{\beta_1}$ is $N(\beta_1, \sigma^2_\hat{\beta_1})$, where the variance of the distribution, $\sigma^2_\hat{\beta_1}$, is 

\[
\sigma^2_\hat{\beta_1} = \frac{1}{n} \frac{var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  var \left(X_i \right)  \right]^2}
\]

The large-sample normal distribution of $\hat{\beta_0}$ is $N(\beta_0, \sigma^2_\hat{\beta_0})$, where
\[
 \sigma^2_\hat{\beta_0} =  \frac{1}{n} \frac{var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 }\text{, where } H_i = 1 - \left[ \frac{\mu_X} {E \left( X_1^2\right)} \right] X_i.
\]

</p>
</div>



```{r}
#Population Regression
pop_n      <- 1000
x          <- runif(pop_n, min = 0, max = 20)
u          <- rnorm(pop_n, sd = 10)
y          <- -2 + 3.5*x + u
population <- data.frame(x, y)



n <- 100 # sample size
sigma_b1 <- var( ( x - mean(x) ) * u ) / (100 * var(x)^2)
H_i      <- 1 - mean(x) / mean(x^2) * x
sigma_b0 <- var(H_i * u) / (n * mean(H_i^2)^2 )

#Estimation
number_of_estimations <- 500


fit <- matrix(ncol = 2, nrow = number_of_estimations)
for (i in 1:number_of_estimations){
 random_sample  <- population[ sample(1:pop_n, n), ]
 fit[i, ]       <- lm(y ~ x, data = random_sample)$coefficients
}

var(fit[ ,1])
hist(fit[ ,1], main = bquote(The ~ distribution  ~ of ~ 500 ~ beta[1] ~ estimates))

var(fit[ ,2])
hist(fit[ ,2], , main = bquote(The ~ distribution  ~ of ~ 500 ~ beta[2] ~ estimates))
```

